#!/usr/bin/env python

"""
Script standardize_mols
==========================
This script is used for standardizing molecules.
"""

# standard
import sys
import warnings
from pathlib import Path
from datetime import datetime
import logging
import argparse
# data
import pandas as pd
# chemoinformatics
import rdkit
from rdkit import RDLogger
from rdkit.Chem.Scaffolds import MurckoScaffold
# dev
import npfc
from npfc import load
from npfc import save
from npfc import utils
from npfc import standardize
from npfc import draw

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def main():

    # init
    d0 = datetime.now()
    description = """Script used for loading molecules from SDF, CSV or HDF files, convert them into RDKit objects and export them into CSV or HDF files.
    Molecules that failed the RDKit conversion have None for structure but their properties, if any, remain.

    It uses the installed npfc libary in your favorite env manager.

    Examples:

        >>> # Convert a SDF into a HDF using molecule titles as molecule id
        >>> load_mols file_in.sdf file_out.hdf --src_id _Name
        >>> # Chunk a CSV file into SDF files of 100 randomly ordered records while keeping all properties
        >>> load_mols file_in.csv file_out.sdf -n 100 -k True -s True --src_id prop1 --src_mol mol --out_id _Name
    """

    # parameters CLI

    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('input_mols', type=str, default=None, help="Input file.")
    parser.add_argument('-o', '--output-template', type=str, default=None, help="Output name to use as template for outputs (i.e. tests/output.csv.gz would be tests/output_passed.csv.gz). If none is specified then input_mols is used instead.")
    parser.add_argument('-p', '--protocol', type=str, default=None, help="Configuration file in JSON for specifying a standardization protocol. See the docs for the default protocol.")
    parser.add_argument('-r', '--ref-file', type=str, default=None, help="Reference file for identifying duplicate molecules. If none is specified, then one ref file is created in the output folder. For Murcko scaffolds, a new ref file is created with suffix _murcko in case keep_intermediates is True.")
    parser.add_argument('-d', '--decode-mols', type=bool, default=True, help="Decode molecules into base64 strings for input.")
    parser.add_argument('-e', '--encode-mols', type=bool, default=True, help="Encode molecules into base64 strings for output.")
    parser.add_argument('-m', '--murcko', type=bool, default=False, help="Extract Murcko scaffolds from passed molecules. Another round of standardization is performed afterwards.")
    parser.add_argument('-k', '--keep-intermediates', type=bool, default=False, help="Keep results from first standardization.")
    parser.add_argument('-c', '--clear', type=bool, default=False, help="Clear previous run outputs. Useful for debugging!")
    parser.add_argument('--in_id', type=str, default='idm', help="Identifier column in the source file.")
    parser.add_argument('--in_mol', type=str, default='mol', help="Molecule column in the source file.")
    parser.add_argument('--in_sep', type=str, default='|', help="Separator to use in case the input file is a csv.")
    parser.add_argument('--out_id', type=str, default='idm', help="Identifier column in the output file.")
    parser.add_argument('--out_mol', type=str, default='mol', help="Molecule column in the output file.")
    parser.add_argument('--out_sep', type=str, default='|', help="Separator to use in case the output file is a csv.")
    parser.add_argument('--compute_2D', type=bool, default=False, help="Compute 2D coordinates.")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()

    # logging

    logger = utils._configure_logger(args.log)
    logger.info("RUNNING STANDARDIZE_MOLS")
    warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)  # if None is returned instead of a molecule, do not complain about mixed types
    pad = 40
    lg = RDLogger.logger()
    lg.setLevel(RDLogger.CRITICAL)

    # parse arguments

    # check on args values not already checked by argparse
    if args.output_template is None:
        logger.warning("OUTPUT_TEMPLATE NOT DEFINED, USING INPUT_MOLS INSTEAD")
        output_template = args.input_mols
    else:
        output_template = args.output_template
    output_dir = Path(output_template).resolve().parent
    utils.check_arg_input_file(args.input_mols)
    utils.check_arg_output_file(output_template)
    utils.check_arg_config_file(args.protocol)
    if args.protocol is None:
        logger.warning("PROTOCOL IS NOT DEFINED, USING DEFAULT")
    # in infos
    in_format, in_compression = utils.get_file_format(args.input_mols)
    # out infos
    path_output_template = Path(output_template)

    out_suffixes = path_output_template.suffixes
    out_format, out_compression = utils.get_file_format(output_template)
    # define outputs
    output_basename = str(output_dir) + "/" + path_output_template.stem.split(".")[0]
    output_passed = output_basename + "_passed" + ''.join(out_suffixes)
    output_filtered = output_basename + "_filtered" + ''.join(out_suffixes)
    output_error = output_basename + "_error" + ''.join(out_suffixes)
    if args.ref_file is None:
        output_ref = output_basename + "_ref" + '.hdf'
        logger.warning(f"REF_FILE NOT DEFINED, SETTING IT TO '{output_ref}'")
    else:
        output_ref = args.ref_file
    utils.check_arg_output_file(output_ref)

    # outputs in case of murcko scaffold
    output_passed_intermediate = output_basename + "_passed_mols" + ''.join(out_suffixes)
    output_filtered_intermediate = output_basename + "_filtered_mols" + ''.join(out_suffixes)
    output_error_intermediate = output_basename + "_error_mols" + ''.join(out_suffixes)
    output_ref_path = Path(output_ref)
    output_ref_key = output_ref_path.stem.split('.')[0]
    ref_file_intermediate = str(output_ref_path.resolve().parent) + "/" + output_ref_key + '_mols' + ''.join(output_ref_path.suffixes)

    # clear previous runs if any
    if args.clear:
        logger.warning(f"CLEARING PREVIOUS OUTPUT FILES")
        files = (output_passed_intermediate, output_filtered_intermediate,
                 output_error_intermediate, ref_file_intermediate,
                 output_passed, output_filtered, output_error, output_ref,
                 )
        [Path(f).unlink() for f in files if Path(f).is_file()]

    # display infos

    # versions
    logger.info("LIBRARY VERSIONS:")
    logger.info("rdkit".ljust(pad) + f"{rdkit.__version__}")
    logger.info("pandas".ljust(pad) + f"{pd.__version__}")
    logger.info("npfc".ljust(pad) + f"{npfc.__version__}")
    # arguments
    logger.info("ARGUMENTS:")
    logging.info("INPUT_MOLS".ljust(pad) + f"{args.input_mols}")
    logging.info("OUTPUT_TEMPLATE".ljust(pad) + f"{output_template}")
    logging.info("OUTPUT_DIR".ljust(pad) + f"{output_dir}")
    if args.protocol is None:
        logging.info("PROTOCOL".ljust(pad) + "DEFAULT")
    else:
        logging.info("PROTOCOL".ljust(pad) + f"{args.protocol}")
    logging.info("REF_FILE".ljust(pad) + f"{output_ref}")
    logging.info("ENCODE_MOLS".ljust(pad) + f"{args.encode_mols}")
    logging.info("DECODE_MOLS".ljust(pad) + f"{args.decode_mols}")
    logging.info("IN_ID".ljust(pad) + f"{args.in_id}")
    logging.info("IN_MOL".ljust(pad) + f"{args.in_mol}")
    logging.info("IN_FORMAT".ljust(pad) + f"{in_format}")
    logging.info("IN_COMPRESSION".ljust(pad) + f"{in_compression}")
    logging.info("OUT_MOL".ljust(pad) + f"{args.out_mol}")
    logging.info("OUT_ID".ljust(pad) + f"{args.out_id}")
    logging.info("OUT_FORMAT".ljust(pad) + f"{out_format}")
    logging.info("OUT_COMPRESSION".ljust(pad) + f"{out_compression}")
    logging.info("CLEAR".ljust(pad) + f"{args.clear}")
    logging.info("LOG".ljust(pad) + f"{args.log}")
    # display outputs
    logger.info("OUTPUT FILES:")
    logger.info("OUTPUT_PASSED".ljust(pad) + f"{output_passed}")
    logger.info("OUTPUT_FILTERED".ljust(pad) + f"{output_filtered}")
    logger.info("OUTPUT_ERROR".ljust(pad) + f"{output_error}")
    logger.info("OUTPUT_REF".ljust(pad) + f"{output_ref}")
    if args.murcko:
        logging.info("INTERDMEDIATE RESULTS:")
        logging.info("OUTPUT_PASSED".ljust(pad) + f"{output_passed_intermediate}")
        logging.info("OUTPUT_FILTERED".ljust(pad) + f"{output_filtered_intermediate}")
        logging.info("OUTPUT_ERROR".ljust(pad) + f"{output_error_intermediate}")
        logging.info("OUTPUT_REF".ljust(pad) + f"{ref_file_intermediate}")
    # begin
    logging.info("BEGIN")

    # load mols
    logging.info("LOADING MOLECULES")
    d1 = datetime.now()
    df_mols = load.file(args.input_mols,
                        in_id=args.in_id,
                        out_id=args.out_id,
                        in_mol=args.in_mol,
                        out_mol=args.out_mol,
                        keep_props=True,
                        decode=args.decode_mols,
                        in_sep=args.in_sep
                        )
    num_failed = df_mols['mol'].isna().sum()
    logging.info(f"LOADED {len(df_mols)} RECORDS WITH {num_failed} FAILURE(S)")

    # standardize molecules

    # describe protocol
    d2 = datetime.now()

    logger.info(f"STANDARDIZING MOLECULES")
    logger.info(f"PROTOCOL:")
    s = standardize.Standardizer(ref_file=output_ref)
    if args.protocol is not None:
        s.protocol = args.protocol
    # abbreviations = task_abbreviation(s.protcol['tasks'])
    [logger.info(f"TASK #{str(i+1).zfill(2)} {task}") for i, task in enumerate(s._protocol['tasks'])]
    [logger.info(f"OPTION {opt}".ljust(pad) + f"{value}") for opt, value in s._protocol.items() if opt != 'tasks']
    logger.info("TIMEOUT FOR ABOVE TASKS".ljust(pad) + f"{standardize.TIMEOUT} SEC")
    logger.info(f"FILTER DUPLICATES IS SET TO TRUE")
    # run protocol
    logger.info(f"RUN STANDARDIZATION")
    df_passed, df_filtered, df_error = s.run_df(df_mols)

    logger.info(f"RESULTS:")
    num_passed = len(df_passed.index)
    num_filtered = len(df_filtered.index)
    num_error = len(df_error.index)
    logger.info(''.join([f"{header}".ljust(10) for header in ('HEADER', 'PASSED', 'FILTERED', 'ERROR')]))
    logger.info(''.join([f"{header}".ljust(10) for header in ('COUNT', num_passed, num_filtered, num_error)]))
    d3 = datetime.now()

    # Extract Murcko scaffolds
    if args.murcko:

        if args.keep_intermediates:
            logger.info(f"SAVING INTERMEDIATE OUTPUTS:")
            # passed
            outputs = save.file(df_passed, output_passed_intermediate)
            logger.info(f"SAVED {outputs[0][1]} RECORDS AT {outputs[0][0]}")
            # filtered
            outputs = save.file(df_filtered, output_filtered_intermediate)
            logger.info(f"SAVED {outputs[0][1]} RECORDS AT {outputs[0][0]}")
            # error
            outputs = save.file(df_error, output_error_intermediate)
            logger.info(f"SAVED {outputs[0][1]} RECORDS AT {outputs[0][0]}")
            # ref
            Path(output_ref).rename(ref_file_intermediate)
            with pd.HDFStore(ref_file_intermediate) as store:
                store.get_node(output_ref_key) ._f_rename(output_ref_key + '_mols')
        else:
            logger.info(f"REMOVING {output_ref}")
            Path(output_ref).unlink()

        logger.info(f"EXTRACTING MURCKO SCAFFOLDS")
        d4 = datetime.now()
        df_mols = df_passed.copy()
        df_mols[args.out_mol] = df_mols[args.out_mol].map(MurckoScaffold.GetScaffoldForMol)

        logger.info(f"STANDARDIZING MURCKO SCAFFOLDS")
        d5 = datetime.now()
        # prepare df for being processed by standardizer
        df_mols['__tmp__'] = df_mols.index
        df_mols.index = range(len(df_mols.index))
        df_mols.rename({'__tmp__': args.out_id}, axis=1, inplace=True)
        # run standardization
        df_passed, df_filtered, df_error = s.run_df(df_mols)
        logger.info(f"RESULTS FOR MURCKO SCAFFOLDS:")
        num_passed = len(df_passed.index)
        num_filtered = len(df_filtered.index)
        num_error = len(df_error.index)
        logger.info(''.join([f"{header}".ljust(10) for header in ('HEADER', 'PASSED', 'FILTERED', 'ERROR')]))
        logger.info(''.join([f"{header}".ljust(10) for header in ('COUNT', num_passed, num_filtered, num_error)]))

    # Compute 2D coordinates
    d6 = datetime.now()
    if args.compute_2D:
        logger.info(f"COMPUTING 2D COORDINATES")
        df_passed[args.out_mol] = df_passed[args.out_mol].map(draw.compute_2D)

    d7 = datetime.now()
    logger.info(f"SAVING OUTPUTS")
    # passed
    outputs = save.file(df_passed, output_passed)
    logger.info(f"SAVED {outputs[0][1]} RECORDS AT {outputs[0][0]}")
    # filtered
    outputs = save.file(df_filtered, output_filtered)
    logger.info(f"SAVED {outputs[0][1]} RECORDS AT {outputs[0][0]}")
    # error
    outputs = save.file(df_error, output_error)
    logger.info(f"SAVED {outputs[0][1]} RECORDS AT {outputs[0][0]}")
    d8 = datetime.now()

    # end

    logging.info("SUMMARY")
    logger.info("COMPUTATIONAL TIME: CONFIGURING JOB".ljust(pad * 2) + f"{d1-d0}")
    logger.info("COMPUTATIONAL TIME: LOADING MOLECULES".ljust(pad * 2) + f"{d2-d1}")
    logger.info("COMPUTATIONAL TIME: STANDARDIZING MOLECULES".ljust(pad * 2) + f"{d3-d2}")
    if args.murcko:
        if args.keep_intermediates:
            logger.info("COMPUTATIONAL TIME: SAVING INTERMEDIATE OUTPUTS".ljust(pad * 2) + f"{d4-d3}")
        logger.info("COMPUTATIONAL TIME: EXTRACTING MS".ljust(pad * 2) + f"{d5-d4}")
        logger.info("COMPUTATIONAL TIME: STANDARDIZING MS".ljust(pad * 2) + f"{d6-d5}")
    if args.compute_2D:
        logger.info("COMPUTATIONAL TIME: COMPUTING 2D".ljust(pad * 2) + f"{d7-d6}")
    logger.info("COMPUTATIONAL TIME: SAVING OUTPUTS".ljust(pad * 2) + f"{d8-d7}")
    logger.info("COMPUTATIONAL TIME: TOTAL".ljust(pad * 2) + f"{d8-d0}")
    logging.info("END")
    sys.exit(0)


if __name__ == '__main__':
    main()
