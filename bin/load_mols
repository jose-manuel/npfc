#!/usr/bin/env python

"""
Script load_mols
==========================
This script is used for loading molecules from SDF, CSV or HDF files and then
export them to CSV or HDF files with RDKit Mol objects.
"""

# standard
import sys
import warnings
from pathlib import Path
from datetime import datetime
import logging
import argparse
# data
import pandas as pd
# chemoinformatics
import rdkit
from rdkit import RDLogger
# custom libraries
import npfc
from npfc import load
from npfc import save
from npfc import utils


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def main():

    # init
    d0 = datetime.now()
    description = """Script used for loading molecules from SDF, CSV or HDF files, convert them into RDKit objects and export them into CSV or HDF files.
    Molecules that failed the RDKit conversion have None for structure but their properties, if any, remain.

    It uses the installed npfc libary in your favorite env manager.

    Examples:

        >>> # Convert a SDF into a HDF using molecule titles as molecule id
        >>> load_mols file_in.sdf file_out.hdf --src_id _Name
        >>> # Chunk a CSV file into SDF files of 100 randomly ordered records while keeping all properties
        >>> load_mols file_in.csv file_out.sdf -n 100 -k True -s True --src_id prop1 --src_mol mol --out_id _Name
    """

    # parameters CLI

    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('input_mols', type=str, default=None, help="Input file.")
    parser.add_argument('output_mols', type=str, default=None, help="Output file. If chunking, then it is used as prefix.")
    parser.add_argument('-n', '--nrecords', type=int, default=None, help="Maximum number of records within a chunk. None means no chunking.")
    parser.add_argument('-s', '--shuffle', type=bool, default=False, help="Shuffle records.")
    parser.add_argument('-k', '--keep_props', type=bool, default=False, help="Keep all properties found in the source, in addition to molid and mol columns.")
    parser.add_argument('-e', '--encode_mols', type=bool, default=True, help="Encode molecules into base64 strings.")
    parser.add_argument('-d', '--decode_mols', type=bool, default=True, help="Decode molecules into base64 strings.")
    parser.add_argument('--src_id', type=str, default='idm', help="Identifier column in the source file.")
    parser.add_argument('--src_mol', type=str, default='mol', help="Molecule column in the source file.")
    parser.add_argument('--src_sep', type=str, default='|', help="Separator to use in case the input file is a csv.")
    parser.add_argument('--src_lists', type=str, default=[], help="Columns to convert from str to list if the input file is a csv.")
    parser.add_argument('--out_id', type=str, default='idm', help="Identifier column in the output file.")
    parser.add_argument('--out_mol', type=str, default='mol', help="Molecule column in the output file.")
    parser.add_argument('--col_sep', type=str, default='|', help="Separator to use in case the output file is a csv.")
    parser.add_argument('--random_seed', type=int, default=None, help="Value to specify to make shuffling reproducible.")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()

    # logging

    logger = utils._configure_logger(args.log)
    logger.info("RUNNING LOAD_MOLS")
    warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)  # if None is returned instead of a molecule, do not complain about mixed types
    pad = 40
    lg = RDLogger.logger()
    lg.setLevel(RDLogger.CRITICAL)

    # parse arguments

    # check on args values not already checked by argparse
    utils.check_arg_input_file(args.input_mols)
    utils.check_arg_output_file(args.output_mols)
    utils.check_arg_positive_number(args.nrecords)
    # src infos
    src_suffixes = Path(args.input_mols).suffixes
    src_format = src_suffixes[0]
    if len(src_suffixes) > 1:
        src_compression = utils.get_conversion(src_suffixes[1])
    else:
        src_compression = None
    # out infos
    out_suffixes = Path(args.output_mols).suffixes
    out_format = out_suffixes[0]
    if len(out_suffixes) > 1:
        out_compression = utils.get_conversion(out_suffixes[1])
    else:
        out_compression = None

    # display infos

    # versions
    logger.info("LIBRARY VERSIONS")
    logger.info("rdkit".ljust(pad) + f"{rdkit.__version__}")
    logger.info("pandas".ljust(pad) + f"{pd.__version__}")
    logger.info("npfc".ljust(pad) + f"{npfc.__version__}")
    # arguments
    logger.info("ARGUMENTS")
    logging.info("INPUT_MOLS".ljust(pad) + f"{args.input_mols}")
    logging.info("OUT_MOLS".ljust(pad) + f"{args.input_mols}")
    logging.info("NRECORDS".ljust(pad) + f"{args.nrecords}")
    logging.info("SHUFFLE".ljust(pad) + f"{args.shuffle}")
    logging.info("KEEP_PROPS".ljust(pad) + f"{args.keep_props}")
    logging.info("ENCODE_MOLS".ljust(pad) + f"{args.encode_mols}")
    logging.info("DECODE_MOLS".ljust(pad) + f"{args.decode_mols}")
    logging.info("SRC_ID".ljust(pad) + f"{args.src_id}")
    logging.info("SRC_MOL".ljust(pad) + f"{args.src_mol}")
    logging.info("SRC_FORMAT".ljust(pad) + f"{src_format}")
    logging.info("SRC_COMPRESSION".ljust(pad) + f"{src_compression}")
    logging.info("OUT_MOL".ljust(pad) + f"{args.out_mol}")
    logging.info("OUT_ID".ljust(pad) + f"{args.out_id}")
    logging.info("OUT_FORMAT".ljust(pad) + f"{out_format}")
    logging.info("OUT_COMPRESSION".ljust(pad) + f"{out_compression}")
    logging.info("LOG".ljust(pad) + f"{args.log}")

    # begin
    logging.info("BEGIN")

    # load mols
    logging.info("LOADING MOLECULES")
    d1 = datetime.now()
    if src_format == '.sdf':
        df_mols = load.from_sdf(args.input_mols,
                                col_id=args.src_id,
                                keep_props=args.keep_props,
                                )
    elif src_format == '.csv':
        df_mols = load.from_csv(args.input_mols,
                                col_mol=args.src_mol,
                                keep_props=args.keep_props,
                                decode_mols=args.decode_mols,
                                sep=args.col_sep,
                                cols_list=args.src_lists,
                                )
    else:   # check on argument for input does not leave any other option than sdf, csv or hdf
        df_mols = load.from_hdf(args.input_mols,
                                decode_mols=args.decode_mols,
                                col_mol=args.src_mol,
                                )

    num_failed = df_mols['mol'].isna().sum()
    logging.info(f"LOADED {len(df_mols)} RECORDS WITH {num_failed} FAILURE(S)")

    # save mols
    d2 = datetime.now()
    logging.info("SAVING MOLECULES")
    outputs_files = save.save(df_mols,
                              args.output_mols,
                              col_mol=args.out_mol,
                              col_id=args.out_id,
                              chunk_size=args.nrecords,
                              shuffle=args.shuffle,
                              encode_mols=args.encode_mols,
                              random_seed=args.random_seed,
                              )
    # display results
    for output in outputs_files:
        logging.info(f"SAVED {output[1]} RECORDS AT {output[0]}")
    d3 = datetime.now()

    # end

    logging.info("END")
    logger.info("COMPUTATIONAL TIME: CONFIGURING JOB".ljust(pad * 2) + f"{d1-d0}")
    logger.info("COMPUTATIONAL TIME: LOADING MOLECULES".ljust(pad * 2) + f"{d2-d1}")
    logger.info("COMPUTATIONAL TIME: SAVING MOLECULES".ljust(pad * 2) + f"{d3-d2}")
    logger.info("COMPUTATIONAL TIME: TOTAL".ljust(pad * 2) + f"{d3-d0}")

    sys.exit(0)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


if __name__ == '__main__':
    main()
