#!/usr/bin/env python

"""
Script report_protocol
==========================
This script is used for generating CSV files and their corresponding plots for
reporting a NPFC run on a given dataset.

The folder tree:
"""

# standard
import warnings
import logging
import argparse
from datetime import datetime
from collections import Counter
import re
from pathlib import Path
from collections import OrderedDict
# data handling
import json
import pandas as pd
from pandas import DataFrame
import networkx as nx
# data visualization
from matplotlib import pyplot as plt
import seaborn as sns
from matplotlib.ticker import FuncFormatter
from pylab import savefig
# chemoinformatics
import rdkit
from rdkit import Chem
# docs
from typing import List
from typing import Tuple
from rdkit import RDLogger
# custom libraries
import npfc
from npfc import utils
from npfc import load
from npfc import save
from npfc import fragment_combination


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def print_title(title: str, logger: logging.Logger, pad: int):
    """Print a title with padding.

    :param title: title to print
    :param logger: the Logger object currently used for coherent display within the log
    :param pad: a padding added for highlighting the title ("="*pad)
    """
    pad = pad * 2
    logger.info("=" * pad)
    logger.info(title.upper().center(pad))
    logger.info("=" * pad)


def find_wd_levels(WD: str, natref: str = None, frags: str = None) -> dict:
    """Starting from specified WD, iterate over subfolders using globbing to find
    the three possible levels of working directories:

        - preprocess: the WD where all the preprocessing of the dataset is performed
        - natref: the WDs for synthetic datasets specifying reference natural datasets (contains only the subset step)
        - frags: the WDs where the results involving fragments are stored. Concerns: fragment search, fragment_combination, fragment_graph, pnp and fragment_tree.

    :param WD: the working directory where to look for levels. If not the data folder, the suffix data is appended.
    :param natref: the natref subdir to use for reporting. If None, all natref subdirs are considered.
    :param frags: the frags subdir to use for reporting. If None, all frags subdirs are considered.
    :return: a dictionary with keys preprocess, natref and frags and values the found working directories. For preprocess, only one wd can be found for each dataset, but a list was used for easier iteration over keys.
    """

    # preprocess wd
    if Path(WD).name == 'data':
        wd_preprocess = [WD]
    else:
        if not Path(f"{WD}/data").exists():
            raise ValueError(f"Error! '{WD}/data' could not be found!")
        wd_preprocess = [f"{WD}/data".replace('//', '/')]
    # find natref dirs
    if natref is None:
        wd_natref = [str(p) for p in list(Path(WD).glob('**')) if re.match('natref_.*', Path(p).name)]
    else:
        wd_natref = [str(p) for p in list(Path(WD).glob('**')) if Path(p).name == natref]
    # find frags dirs
    if frags is None:
        wd_frags = [str(p) for p in list(Path(WD).glob('**')) if re.match('frags_.*', Path(p).name)]
    else:
        wd_frags = [str(p) for p in list(Path(WD).glob('**')) if Path(p).name == frags]
    # consider only wanted natref subdirs
    if natref is not None:
        wd_frags = [wd for wd in wd_frags if Path(wd).parent.name == natref]

    return {'preprocess': wd_preprocess, 'natref': wd_natref, 'frags': wd_frags}


def find_wd_level_steps(wd_level: str, pattern: str = '[0-9][0-9]_.*') -> list:
    """Return the list of all steps root folders of current working directory level for one subdir (i.e. d['fragments'][0], d['natref'][2], etc.).
    Each subdir consists in a different natref or fragment dataset used.

    :param wd_level: the root folder of a given step performed during a npfc protocol
    :param pattern: the pattern to use for identiying step folders
    :return: the list of step folders found in the specified level dataset.
    """
    # find all steps following synthax pattern
    return [str(p) for p in list(Path(wd_level).glob('*')) if re.match(pattern, Path(p).name)]


def _get_missing_chunkids(WD: Path, subfolders: list, chunks_ini_id: list, check_on: str) -> OrderedDict:
    """Iterate over defined subfolders in WD and compare the number of chunks found in sub-subfolder check_on (data or log) to check
    if files are missing. An OrderedDict is computed with for each key the subfolder and as values the list of missing chunk ids.

    :param WD: the WD
    :param subfolders: the list of subfolders to check (02_load, 03_deglyco, etc.). For any subfolder with '_std' in its name, then number of counted chunks if checking on data will be a third (because there are 3 outputs per chunk: passed, filtered and error).
    :param: chunks_ini_id: the list of chunk ids defined in the step chunk
    """
    # init
    errors = OrderedDict()
    steps = [s.split('_')[1] for s in subfolders]
    # no chunks
    if len(chunks_ini_id) == 0:  # so no reference, just look if there are the expected number of outputs per subfolder
        logger.info("NO CHUNK SUBFOLDER COULD BE FOUND, ASSUMING THIS IS A FRAGMENTS DATASET")
        for step, sf in zip(steps, subfolders):
            error = False
            sf_output_files = [s.name for s in WD.glob(f"{sf}/{check_on}/*")]
            if len(sf_output_files) < 1:
                error = True
            if check_on == 'data':
                if ('_std' in sf and len(sf_output_files) < 3) or (not '_std' in sf and len(sf_output_files) < 1):
                    error = True
                else:
                    error = False
            if error:
                errors[step] = ["Missing output files!"]
    else:
        # chunks
        for step, sf in zip(steps, subfolders):
            sf_chunks = [s.name for s in WD.glob(f"{sf}/{check_on}/*")]
            if sf == '02_load':
                sf_chunks_id = sorted([s.split("_")[-1] for s in sf_chunks])
            else:
                sf_chunks_id = sorted([s.split("_")[-2] for s in sf_chunks])
            # check number of chunks to see if there is a problem, 3 output chunks for std so speical case
            if check_on == 'data':
                if ('_std' in sf and (len(sf_chunks_id)/3) < len(chunks_ini_id)) or (not '_std' in sf and len(sf_chunks_id) < len(chunks_ini_id)):
                    error = True
                else:
                    error = False
            # no this problem with log files
            else:
                if len(sf_chunks_id) < len(chunks_ini_id):
                    error = True
                else:
                    error = False

            if error:
                errors[step] = [x for x in chunks_ini_id if x not in sf_chunks_id]

    return errors


def get_chunk_ref_count(wd_levels):
    wd = wd_levels['preprocess'][0]
    chunks = [str(x) for x in list(Path(wd).glob('[0-9][0-9]_chunk/data/*'))]
    logging.debug(f"GET_CHUNKS_REF_COUNT - IDENTIFIED CHUNKS:\n" + '\n'.join(chunks))
    return len(chunks)


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ BEGIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


if __name__ == '__main__':

    d0 = datetime.now()
    parser = argparse.ArgumentParser(description="Compute all required files for analyzing FCC results", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('wd', type=str, default='./', help="Working directory from the job to analyse (root folder, where the smk file is)")
    parser.add_argument('-n', '--natref', type=str, default=None, help="The name of the natref subdir to focus the report on. If None, then all identified natref subdirs are considered for reporting. Requires the natref_ prefix.")
    parser.add_argument('-f', '--frags', type=str, default=None, help="The name of the frags subdir to focus the report on. If None, then all identified natref subdirs are considered for reporting. Requires the frags_ prefix.")
    parser.add_argument('-d', '--dataset', type=str, default=None, help="Dataset name for using in the csv/png outputs in the report folder.")
    parser.add_argument('-p', '--prefix', type=str, default=None, help="Prefix used for output files in the data/log folders.")
    parser.add_argument('-c', '--color', type=str, default='', help="Prefix used for output files in the data/log folders.")
    parser.add_argument('--csv', type=str, default=False, help="Generate only CSV output files")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()


    # logging
    logger = utils._configure_logger(args.log)
    logger.info("PLEASE RUN THIS COMMAND FROM THE DATA FOLDER OF THE DATASET TO PARSE (i.e. natural/dnp/data)")
    logger.info("RUNNING PLOT_PIPELINE_RESULTS")
    warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)  # if None is returned instead of a molecule, do not complain about mixed types
    pd.options.mode.chained_assignment = None  # disable pd.io.pytables.SettingWithCopyWarning
    pad = 40
    lg = RDLogger.logger()
    lg.setLevel(RDLogger.INFO)

    logger.info("LIBRARY VERSIONS:")
    logger.info("rdkit".ljust(pad) + f"{rdkit.__version__}")
    logger.info("pandas".ljust(pad) + f"{pd.__version__}")
    logger.info("npfc".ljust(pad) + f"{npfc.__version__}")
    logger.info("seaborn".ljust(pad) + f"{sns.__version__}")

    logger.info("ARGUMENTS:")
    logger.info("WD".ljust(pad) + f"{args.wd}")
    logger.info("NATREF".ljust(pad) + f"{args.natref}")
    logger.info("FRAGS".ljust(pad) + f"{args.frags}")


    logger.info("DATASET".ljust(pad) + f"{args.dataset}")
    logger.info("PREFIX".ljust(pad) + f"{args.prefix}")
    logger.info("COLOR".ljust(pad) + f"{args.color}")
    logger.info("COMPUTE CSV ONLY".ljust(pad) + f"{args.csv}")

    # check arguments
    utils.check_arg_input_dir(args.wd)
    wd_levels = find_wd_levels(args.wd, natref=args.natref, frags=args.frags)
    # natref
    if args.natref is not None and args.natref[0:7] != 'natref_':
        raise ValueError(f"ERROR! NATREF PREFIX IS EITHER WRONG OR MISSING! ('{args.natref}')")
    # frags
    if args.frags is not None and args.frags[0:6] != 'frags_':
        raise ValueError(f"ERROR! FRAGS PREFIX IS EITHER WRONG OR MISSING! ('{args.frags}')")


    # init variables

    # Begin iteration on folder tree
    chunk_ref_count = get_chunk_ref_count(wd_levels)

    # the color is applied to the dataset itself, but since the coloring will be performed during the processing of the
    # I like to have my very own palette of colors but have a hard time remembering hexadecimal codes
    if args.color == 'gray':
        color = '#808080'
    elif args.color == 'green':
        color = '#2CA02C'
    elif args.color == 'blue':
        color = '#378EBF'
    elif args.color == 'red':
        color = '#EB5763'
    else:
        color = args.color

    # exclude folders to analyze


    # preprocess - natref - frags
    for wd_level in wd_levels:
        print(f"\n{wd_level}")
        for wd_subdir in wd_levels[wd_level]:
            wd_steps = find_wd_level_steps(wd_subdir)
            print('\n'.join(wd_steps))
