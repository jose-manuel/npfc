#!/usr/bin/env python

"""
Script plot_pipeline_results
==========================
This script is used for generating CSV files for plotting the results produced
by the FCC workflow. The root folder of the current job has to be specified
(Where the snakefile is located).

It will browse every subfolder and generate results in a plots subfolder in the
root folder.
"""

# standard
import warnings
import argparse
from datetime import datetime
from collections import Counter
import re
from pathlib import Path
from collections import OrderedDict
# data handling
import json
import pandas as pd
from pandas import DataFrame
# data visualization
from matplotlib import pyplot as plt
import seaborn as sns
from matplotlib.ticker import FuncFormatter
from pylab import savefig
# chemoinformatics
import rdkit
from rdkit import Chem
from rdkit import RDLogger
# custom libraries
import npfc
from npfc import utils
from npfc import load


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def _get_chunks(WD, pattern):
    WD = Path(WD)
    pattern = re.compile(pattern)

    chunks = [str(x) for x in list(WD.glob("*"))]
    chunks = list(filter(pattern.match, chunks))
    chunks.sort()

    return chunks


def save_barplot(df, output_png, x_name, y_name, title, x_label=None, y_label=None, rotate_x=0, perc_labels=None, perc_label_size=15, color='#2ca02c', fig_size=(12, 12)):
    # delete existing file for preventing stacking of plots
    p = Path(output_png)
    if p.exists():
        p.unlink()

    # general style for plotting
    # sns.set(rc={'figure.figsize': fig_size})
    plt.figure(figsize=fig_size)
    sns.set_style('whitegrid', {'axes.edgecolor': '0.2'})
    sns.set_context("paper", font_scale=2)
    # barplot
    svm = sns.barplot(x=df.index, y=df[y_name], color=color)
    svm.set_title(title, fontsize=24, y=1.02)
    svm.tick_params(labelsize=20)
    svm.tick_params(axis='x', rotation=rotate_x)
    svm.set_xticklabels(df[x_name])
    svm.set_xlabel(x_label,fontsize=25, labelpad=20)
    svm.set_ylabel(y_label,fontsize=25, labelpad=20)
    # if no entry, then y ticks get all confused
    if df[y_name].sum() == 0:
        svm.set_yticks((0, 1, 2, 3, 4, 5))
        svm.set_ylim((0, 5))

    # format y labels
    ylabels = ['{:,.0f}'.format(x) for x in svm.get_yticks()]
    svm.set_yticklabels(ylabels)
    # add percentage annotations
    if perc_labels is not None:
        for i in range(len(df.index)):
            row = df.iloc[i]
            svm.text(row.name, row[y_name], row[perc_labels], color='black', ha='center', fontdict={'fontsize': perc_label_size})
    # save
    figure = svm.get_figure()
    if rotate_x > 0:
        figure.subplots_adjust(bottom=0.2)
    figure.savefig(output_png, dpi=600)
    plt.clf()

    return figure


def save_kdeplot(df, output_png, x_name, title, x_label=None, y_label=None, color='#2ca02c', fig_size=(12, 12)):

    # general style for plotting
    sns.set(rc={'figure.figsize': fig_size})
    sns.set_style('whitegrid', {'axes.edgecolor': '0.2'})
    sns.set_context("paper", font_scale=2)

    svm = sns.kdeplot(df['Perc_Frag_Coverage_Per_Mol'], shade=True, label='', color=color)
    svm.set_title(title, fontsize=24, y=1.02)
    svm.tick_params(labelsize=20)
    svm.tick_params(axis='x', rotation=0)
    svm.set_xlim(0, 100)
    #svm.set_xticklabels(df[x_name])
    svm.set_xlabel(x_label,fontsize=25, labelpad=20)
    svm.set_ylabel(y_label,fontsize=25, labelpad=20)

    # save
    figure = svm.get_figure()
    figure.savefig(output_png, dpi=600)
    plt.clf()

    return figure



def check_chunks(WD: str):
    """This functions will look up the expected amount of chunks produced by the
    "1_input" step and verify if all chunks are present in subsequent tasks.
    If not, it will raise a ValueError.

    Subfolders 0_raw and 1_input are excluded from the checks.

    :param WD: Working Directory of the pipeline to check (parent folder of the smk file)
    :return: True if no error was found, ValueError otherwise
    """
    # go inside data subfolder in WD
    WD = Path(WD + "data")

    # initialize the references for comparing
    pattern = "1_input/data/*([0-9][0-9][0-9])?.sdf.gz"
    chunks_ini = [str(s).split("_")[-1].split(".")[0] for s in list(WD.glob(pattern))]
    num_chunks_ini = len(chunks_ini)
    # define what subfolders are to be checked
    subfolders = [str(d).split("/")[-1] for d in WD.iterdir() if d.is_dir() if str(d).split("/")[-1] not in ("0_raw", "1_input")]

    # record errors in case of missing chunks
    errors = OrderedDict()
    for sf in subfolders:
        chunks_sf = [str(s).split("/")[-1] for s in list(WD.glob(str(sf) + "/data/*[0-9][0-9][0-9]*.gz"))]
        # currently no suffix is appended because of retrocompatibility issues
        if sf == "2_load":
            chunks_sf = [s.split("_")[-1] for s in chunks_sf]
        else:
            chunks_sf = [s.split("_")[-2] for s in chunks_sf]
        # check if the number of chunks match
        if len(chunks_sf) < num_chunks_ini:
            errors[sf] = [csf for csf in chunks_ini if csf not in chunks_sf]
    # raise a ValueError if any chunk is missing, describe which ones
    if errors:  # empty dicts are evaluated to False
        msg = "Error! Missing chunks found:\n"
        for sf in errors.keys():
            msg += f"{sf}".ljust(12) + f" ({len(errors[sf])}):\n".rjust(3) + ", ".join([c for c in errors[sf]]) + "\n"
        raise ValueError(msg)

    return True


def get_result_load(WD: str = None, output_csv: str = None):

    logger.info("PLOT_RESULT_LOAD")

    # go inside log subfolder in WD
    WD_LOG = WD + "log"
    logger.info("GENERAL RESULTS")
    logger.info(f"WD={WD_LOG}")


    # iterate over the log files to count status
    pattern = ".*([0-9]{3})?.log"
    chunks = _get_chunks(WD_LOG, pattern)
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_tot = 0
    num_passed = 0
    num_errors = 0

    for c in chunks:
        df = pd.read_csv(c, sep="@", header=None)  # char not found in the log file so we can extract all lines as one column
        records = df[df[0].str.contains("FAILURE")].iloc[0][0].split()
        total = int(records[6])
        errors = int(records[9])
        passed = int(df[df[0].str.contains("SAVED")].iloc[0][0].split()[6])
        logger.debug(f"{c} => PASSED: {passed}/{total} ({errors} ERRORS)")
        num_passed += passed
        num_errors += errors
        num_tot += total

    df = pd.DataFrame({"passed": [num_passed], "error": [num_errors]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    if num_tot > 0:
        df['Perc_status'] = df['Count'].map(lambda x: f"{x/num_tot:.2%}")
    else:
        df['Perc_status'] = 'NA'
    # display results
    logger.info(f"RESULTS:\n\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")

    return output_csv


def get_result_deglyco(WD: str = None, output_csv: str = None):
    """This functions computes the CSV/PNG files for checking on the status of the deglycosylation.
    The WD is the subfolder of the deglycosylation (3_deglyco) and is used in case the df param is not specified.
    Since the job is executed by a KNIME workflow, I used a csv file to store the smiles alongside
    the status of the molecule.
    """
    logger.info("PLOT_RESULT_DEGLYCO")

    logger.info("COMPUTING DF_DEGLYCO FROM WD")

    # go inside log subfolder in WD
    pattern = ".*([0-9]{3})?_deglyco\.csv"
    WD += "log"
    logger.info(f"WD={WD}")

    chunks = _get_chunks(WD, pattern)
    chunks.sort()
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_tot = 0
    num_unchanged = 0
    num_deglycosylated = 0
    num_failed = 0
    num_error = 0

    # count status
    for c in chunks:
        df = pd.read_csv(c, sep="|")
        num_tot += len(df.index)
        num_unchanged += len(df[df['status'] == 'unchanged'].index)
        num_deglycosylated += len(df[df['status'] == 'deglycosylated'].index)
        num_failed += len(df[df['status'] == 'failed'].index)
        num_error += len(df[df['status'] == 'error'].index)

    logger.info(f"TOTAL NUMBER OF MOLECULES: {num_tot:,d}")
    # create a dataframe with counts
    df = pd.DataFrame({"deglycosylated": [num_deglycosylated], "unchanged": [num_unchanged], "failed": [num_failed], 'error': [num_error]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_status'] = df['Count'].map(lambda x: f"{x/num_tot:.2%}")

    logger.info(f"RESULTS:\n\n{df}\n")
    # export to csv
    if output_csv is not None:
        format, compression = utils.get_file_format(output_csv)
        df.to_csv(output_csv, sep="|", compression=compression, index=False)
        logger.info(f"SAVED RESULTS AT '{output_csv}'")

    return output_csv


def _parse_std_chunks(chunks, tot_mols):
    dfs = []
    for c in chunks:
        df = pd.read_csv(c, sep="|", compression="gzip").groupby("task").count()[['status']].rename({'status': 'Count'}, axis=1)
        if len(df.index) > 0:
            dfs.append(df)

    # if no case was found, return an empty dataframe
    if len(dfs) == 0:
        df = pd.DataFrame([], columns=["Count", "Category", "Perc_status"])
        return df

    # concatenate all dataframes and compute the sum of all counts
    df = pd.concat(dfs)
    df["Category"] = df.index  # I don't know how to group by index!
    df = df.groupby("Category").sum()
    df["Category"] = df.index.map(lambda x: x.replace('filter_', ''))
    df['Perc_status'] = df['Count'].map(lambda x: f"{x/tot_mols:.2%}")

    return df


def get_result_std(WD: str = None, output_csv: str = None):
    """This functions computes the CSV file for checking on the status of the standardization.
    The WD is the subfolder of the standardization (4_std) and it uses the log files for counting
    status.

    .. note:: this function only creates temporary files that serve as input for producing files taking into account other steps as well (load for errors and dupl for filtered: sometimes less is more for barplots)
    """
    logger.info("PLOT_RESULT_STD")
    logger.info("COMPUTING TEMPORARY FILES USING ONLY THE STD STEP. THESE FILES WILL BE UPDATED LATER ON USING OTHER STEPS AS WELL.")

    # go inside log subfolder in WD_STD
    WD_LOG = WD + "log"
    logger.info("TEMP GENERAL RESULTS")
    logger.info(f"WD={WD_LOG}")

    pattern = ".*([0-9]{3})?_std\.log"
    # iterate over the log files to count status
    chunks = _get_chunks(WD_LOG, pattern)
    chunks.sort()
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_tot = 0
    num_passed = 0
    num_filtered = 0
    num_error = 0

    for c in chunks:
        df = pd.read_csv(c, sep="@", header=None)  # char not found in the log file so we can extract all lines as one column
        passed, filtered, error = [int(x) for x in df[df[0].str.contains("COUNT")].iloc[0][0].split()[-3:]]
        num_passed += passed
        num_filtered += filtered
        num_error += error
        num_tot += passed + filtered + error

    df = pd.DataFrame({"passed": [num_passed], "filtered": [num_filtered], "error": [num_error]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_status'] = df['Count'].map(lambda x: f"{x/num_tot:.2%}")

    # display results
    logger.info(f"TEMP RESULTS:\n\n{df}\n")

    # export to csv
    if output_csv is not None:
        format, compression = utils.get_file_format(output_csv)
        df.to_csv(output_csv, sep="|", compression=compression, index=False)
        logger.info(f"SAVED RESULTS AT '{output_csv}'")

    # details

    # go inside data subfolder in WD
    WD_DATA = WD + "data"
    logger.info("DETAILED TEMP RESULTS")
    logger.info(f"WD={WD_DATA}")

    # passed
    logger.info("PASSED")
    perc_passed = f"{num_passed/num_tot:.2%}"
    df_passed = pd.DataFrame([[num_passed, "standardized", perc_passed]], columns=["Count", "Category", "Perc_status"])
    logger.info(f"TEMP RESULTS:\n\n{df_passed}\n")

    output_csv_passed = output_csv.split(".")[0] + "_passed.csv"
    if compression is not None:
        output_csv_passed += ".gz"
    df_passed.to_csv(output_csv_passed, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_passed}'")

    # filtered
    logger.info("FILTERED")
    pattern = ".*([0-9]{3})?_filtered\.csv\.gz"
    # iterate over the log files to count status
    chunks = _get_chunks(WD_DATA, pattern)
    df_filtered = _parse_std_chunks(chunks, num_tot)
    logger.info(f"TEMP RESULTS:\n\n{df_filtered}\n")

    output_csv_filtered = output_csv.split(".")[0] + "_filtered.csv"
    if compression is not None:
        output_csv_filtered += ".gz"
    df_filtered.to_csv(output_csv_filtered, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_filtered}'")

    # error
    logger.info("ERROR")
    pattern = ".*([0-9]{3})?_error\.csv\.gz"
    chunks = _get_chunks(WD_DATA, pattern)
    df_error = _parse_std_chunks(chunks, num_tot)
    logger.info(f"TEMP RESULTS:\n\n{df_error}\n")

    output_csv_error = output_csv.split(".")[0] + "_error.csv"
    if compression is not None:
        output_csv_error += ".gz"
    df_error.to_csv(output_csv_error, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_error}'")


    return {'std': output_csv,
            'std_passed': output_csv_passed,
            'std_filtered': output_csv_filtered,
            'std_error': output_csv_error,
            }


def get_result_dupl(WD: str, output_csv: str):

    logger.info("PLOT_RESULT_DUPL")

    # go inside log subfolder in WD
    WD_LOG = WD + "log"
    logger.info("GENERAL RESULTS")
    logger.info(f"WD={WD_LOG}")


    # iterate over the log files to count status
    pattern = ".*([0-9]{3})?_uni\.log"
    chunks = _get_chunks(WD_LOG, pattern)
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_tot = 0
    num_passed = 0
    num_filtered = 0

    for c in chunks:
        df = pd.read_csv(c, sep="@", header=None)  # char not found in the log file so we can extract all lines as one column
        passed, total = [int(x) for x in df[df[0].str.contains("REMAINING MOLECULES")].iloc[0][0].split("MOLECULES:")[1].split("/")]
        num_passed += passed
        num_filtered += total - passed
        num_tot += total

    df = pd.DataFrame({"passed": [num_passed], "filtered": [num_filtered]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    if num_tot > 0:
        df['Perc_status'] = df['Count'].map(lambda x: f"{x/num_tot:.2%}")
    else:
        df['Perc_status'] = 'NA'
    # display results
    logger.info(f"RESULTS:\n\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")

    return output_csv

def get_result_map(WD: str, output_csv: str, file_frags: str):
    logger.info("PLOT_RESULT_MAP")
    # go inside log subfolder in WD
    logger.info(f"WD={WD}")
    p = Path(WD)

    format, compression = utils.get_file_format(output_csv)
    output_csv_basename = output_csv.split(".csv")[0]
    logger.info(f"OUTPUTS BASENAME: {output_csv_basename}")

    # parse std results (num_mol_tot_std)
    logger.info("SYNTH -- COMPUTING TOTAL NUMBER OF MOLECULES BEFORE SUBSTRUCTURE SEARCH")
    chunks = list(p.glob("data/*_synth/data/*[0-9][0-9][0-9]_synth.csv.gz"))
    logger.info(f"SYNTH -- FOUND {len(chunks):,d} CHUNKS_SYNTH")
    # if no chunks found in synth (DNP, ect.), then use uni instead
    if len(chunks) == 0:
        chunks = list(p.glob("data/*_uni/data/*[0-9][0-9][0-9]_uni.csv.gz"))
        logger.info(f"UNI -- FOUND {len(chunks)} CHUNKS_STD. USING THEM INSTEAD OF CHUNKS_SYNTH")
    num_mol_tot_std = sum([len(pd.read_csv(x, sep='|', compression='gzip').index) for x in chunks])
    logger.info(f"UNI -- TOTAL NUMBER OF MOLECULES: {num_mol_tot_std:,d}")

    # parse sub results
    chunks = list(p.glob("data/*_sub/data/*[0-9][0-9][0-9]_sub.csv.gz"))
    chunks.sort()
    logger.info(f"SUB -- FOUND {len(chunks):,d} CHUNKS_SUB")
    if len(chunks) == 0:
        logger.warning("SUB -- NO SUBSTRUCTURE HIT FOUND, EXITING MAPPING")
        return None
    df_sub = pd.concat([pd.read_csv(x, sep='|', compression='gzip') for x in chunks])
    num_tot_hit_sub = len(df_sub.index)
    logger.info(f"SUB -- TOTAL NUMBER OF SUBSTRUCTURE HITS: {num_tot_hit_sub:,d}")

    # regroup by molecule
    df_sub['count_hit_per_mol'] = df_sub.groupby(['idm', 'idf'])['_aidxf'].transform(len)
    groups_mols_sub = df_sub[["idm", "idf", "count_hit_per_mol"]].groupby("idm")
    num_tot_mol_sub = len(groups_mols_sub)
    logger.info(f"SUB -- TOTAL NUMBER OF MOLECULES WITH >0 NATURAL FRAGMENTS: {num_tot_mol_sub:,d}")

    # entries with > 1 fragments
    num_mol_gt1_sub = len(groups_mols_sub.filter(lambda x: len(x) > 1).groupby("idm").first().index)
    logger.info(f"SUB -- NUMBER OF MOLECULES WITH >1 NATURAL FRAGMENTS: {num_mol_gt1_sub:,d}")

    # entries with 1 fragment
    num_mol_eq1_sub = len(groups_mols_sub.filter(lambda x: len(x) == 1).groupby("idm").first().index)
    logger.info(f"SUB -- NUMBER OF MOLECULES WITH 1 NATURAL FRAGMENT: {num_mol_eq1_sub:,d}")

    # missing entries (0 fragment)
    num_mol_eq0_sub = num_mol_tot_std - num_mol_eq1_sub - num_mol_gt1_sub
    logger.info(f"SUB -- NUMBER OF MOLECULES WITH 0 NATURAL FRAGMENT: {num_mol_eq0_sub:,d}")

    # parse fcc results
    chunks = list(p.glob("data/*_fcc/data/*[0-9][0-9][0-9]_fcc.csv.gz"))
    chunks.sort()
    logger.info(f"FCC -- FOUND {len(chunks):,d} CHUNKS_FCC")
    df_fcc = pd.concat([pd.read_csv(x, sep='|', compression='gzip') for x in chunks])
    num_tot_comb_fcc = len(df_fcc.index)
    logger.info(f"FCC -- TOTAL NUMBER OF COMBINATIONS: {num_tot_comb_fcc:,d}")
    num_tot_mol_fcc = len(df_fcc.groupby('idm').first().index)
    logger.info(f"FCC -- TOTAL NUMBER OF MOLECULES: {num_tot_mol_fcc:,d}")

    # missing molecules (filtered because false positives)
    num_mol_fp_fcc = num_mol_gt1_sub - num_tot_mol_fcc
    logger.info(f"FCC -- NUMBER OF FILTERED MOLECULES BECAUSE OF FP: {num_mol_fp_fcc:,d}")

    # parse map results
    chunks = list(p.glob("data/*_fmap/data/*[0-9][0-9][0-9]_fmap.csv.gz"))
    chunks.sort()
    logger.info(f"MAP -- FOUND {len(chunks):,d} CHUNKS_MAP")
    df_map = pd.concat([pd.read_csv(x, sep="|", compression="gzip") for x in chunks])
    df_map.sort_values(["idm", "nfrags"], ascending=True, inplace=True)  # lowest number of frags per mol so first element of each group has the smallest fm
    num_tot_fm_map = len(df_map.index)
    logger.info(f"MAP -- TOTAL NUMBER OF FRAGMENT MAPS: {num_tot_fm_map:,d}")
    num_tot_mol_map = len(df_map.groupby('idm').first().index)
    logger.info(f"MAP -- TOTAL NUMBER OF MOLECULES: {num_tot_mol_map:,d}")

    # missing molecules (filtered because of overlaps and min/max frags)
    num_mol_fp_map = num_tot_mol_fcc - num_tot_mol_map
    logger.info(f"MAP -- NUMBER OF FILTERED MOLECULES BECAUSE OF FP: {num_mol_fp_map:,d}")

    # fragment counts per fm
    df_sub_counts = df_map.groupby('nfrags').count()[['idm']].rename({'idm': 'NumFragmentMaps'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags": "NumFrags"}, axis=1, inplace=True)
    num_tot = df_sub_counts['NumFragmentMaps'].sum()
    df_sub_counts['Perc_FM'] = df_sub_counts['NumFragmentMaps'].map(lambda x: f"{x/num_tot:.2%}")
    logger.info(f"MAP -- RESULTS -- NUMBER OF FRAGMENTS FOUND PER FRAGMENT MAP:\n\n{df_sub_counts}\n")
    output_csv_map_frag_fm = output_csv_basename + "_map_frag_fm.csv"
    if compression is not None:
        output_csv_map_frag_fm += ".gz"
    df_sub_counts.to_csv(output_csv_map_frag_fm, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_frag_fm}'")

    # unique fragment counts per fm
    df_sub_counts = df_map.groupby('nfrags_u').count()[['idm']].rename({'idm': 'NumFragmentMaps'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags_u": "NumUniqueFrags"}, axis=1, inplace=True)
    num_tot = df_sub_counts['NumFragmentMaps'].sum()
    df_sub_counts['Perc_FM'] = df_sub_counts['NumFragmentMaps'].map(lambda x: f"{x/num_tot:.2%}")
    logger.info(f"MAP -- RESULTS -- NUMBER OF UNIQUE FRAGMENTS FOUND PER FRAGMENT MAP:\n\n{df_sub_counts}\n")
    output_csv_map_frag_fm_u = output_csv_basename + "_map_frag_fm_u.csv"
    if compression is not None:
        output_csv_map_frag_fm_u += ".gz"
    df_sub_counts.to_csv(output_csv_map_frag_fm_u, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_frag_fm_u}'")

    # smallest fm per mol
    df_map.sort_values(["idm", "nfrags"], inplace=True)  # ascending so first element of each idm group has the smallest nfrags
    df_map_mol = df_map.groupby('idm').first()
    df_map_mol.reset_index(inplace=True)
    df_map_mol.rename({"index": "idm"}, axis=True, inplace=True)

    # NumFragsPerMol for n=0 and n=1
    df_sub_counts_lt2 = pd.DataFrame({'NumFrags': [0, 1], 'NumMols': [num_mol_eq0_sub, num_mol_eq1_sub]})
    df_sub_counts_lt2_unique = pd.DataFrame({'NumUniqueFrags': [0, 1], 'NumMols': [num_mol_eq0_sub, num_mol_eq1_sub]})

    # fragment counts per mol
    df_sub_counts = df_map_mol.groupby('nfrags').count()[['idm']].rename({'idm': 'NumMols'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags": "NumFrags"}, axis=1, inplace=True)
    df_sub_counts = pd.concat([df_sub_counts_lt2, df_sub_counts])
    df_sub_counts.reset_index(inplace=True)
    num_tot = df_sub_counts['NumMols'].sum()
    df_sub_counts['Perc_Mols'] = df_sub_counts['NumMols'].map(lambda x: f"{x/num_tot:.2%}")
    logger.info(f"MAP -- RESULTS -- NUMBER OF FRAGMENTS FOUND PER MOLECULE:\n\n{df_sub_counts}\n")
    output_csv_map_frag_mol = output_csv_basename + "_map_frag_mol.csv"
    if compression is not None:
        output_csv_map_frag_mol += ".gz"
    df_sub_counts.to_csv(output_csv_map_frag_mol, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_frag_mol}'")

    # unique fragment counts per mol
    df_sub_counts = df_map_mol.groupby('nfrags_u').count()[['idm']].rename({'idm': 'NumMols'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags_u": "NumUniqueFrags"}, axis=1, inplace=True)
    df_sub_counts = pd.concat([df_sub_counts_lt2_unique, df_sub_counts])
    df_sub_counts.reset_index(inplace=True)
    num_tot = df_sub_counts['NumMols'].sum()
    df_sub_counts['Perc_Mols'] = df_sub_counts['NumMols'].map(lambda x: f"{x/num_tot:.2%}")
    logger.info(f"MAP -- RESULTS -- NUMBER OF UNIQUE FRAGMENTS FOUND PER MOLECULE:\n\n{df_sub_counts}\n")
    output_csv_map_frag_mol_u = output_csv_basename + "_map_frag_mol_u.csv"
    if compression is not None:
        output_csv_map_frag_mol_u += ".gz"
    df_sub_counts.to_csv(output_csv_map_frag_mol_u, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_frag_mol_u}'")

    # molecule coverage by fragments
    df_map["_colormap"] = df_map["_colormap"].map(utils.decode_object)
    df_map["aidxs"] = df_map["_colormap"].map(lambda x: [k for k in x.atoms.keys()])
    groups_mol_map = df_map[['idm', 'hac_mol', 'aidxs']].groupby("idm")
    num_tot_mol_fcc_molcov = len(groups_mol_map)
    logger.info(f"MAP -- FCC -- TOTAL NUMBER OF MOLS WHEN CHECKING COVERAGE PER MOLECULE: {num_tot_mol_fcc_molcov}")
    df_frag_mol_cov = groups_mol_map.agg({'aidxs': 'sum'})
    df_frag_mol_cov["hac_mol"] = groups_mol_map.first()['hac_mol']  # ok instead of joint because groups are in the same order
    df_frag_mol_cov["hac_frags"] = df_frag_mol_cov["aidxs"].map(lambda x: len(list(set(x))))
    df_frag_mol_cov.drop("aidxs", axis=1, inplace=True)
    df_frag_mol_cov["Perc_Frag_Coverage_Per_Mol"] = df_frag_mol_cov.apply(lambda x: f"{x['hac_frags'] / x['hac_mol']:.4%}".replace('%', ''), axis=1)
    df_frag_mol_cov.drop("hac_frags", axis=1, inplace=True)
    output_csv_map_perc_frag_cov_mol = output_csv_basename + "_map_perc_frag_cov_mol.csv"
    if compression is not None:
        output_csv_map_perc_frag_cov_mol += ".gz"
    df_frag_mol_cov[['Perc_Frag_Coverage_Per_Mol']].to_csv(output_csv_map_perc_frag_cov_mol, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_perc_frag_cov_mol}'")
    # and now with couting elements in groups of given % for easier display and quicker analysis
    df_frag_mol_cov.rename({"hac_mol": "NumMols"}, axis=1, inplace=True)
    df_frag_mol_cov_counts = df_frag_mol_cov.groupby("Perc_Frag_Coverage_Per_Mol").count()
    df_frag_mol_cov_counts.reset_index(inplace=True)
    num_tot = df_frag_mol_cov_counts["NumMols"].sum()
    df_frag_mol_cov_counts['Perc_Mols'] = df_frag_mol_cov_counts['NumMols'].map(lambda x: f"{x/num_tot:.1%}")
    logger.info(f"MAP -- RESULTS -- PERC OF FRAGMENT COVERAGE PER MOLECULE:\n\n{df_frag_mol_cov_counts}\n")
    output_csv_map_perc_frag_cov_mol_grouped = output_csv_basename + "_map_perc_frag_cov_mol_grouped.csv"
    if compression is not None:
        output_csv_map_perc_frag_cov_mol_grouped += ".gz"
    df_frag_mol_cov_counts.to_csv(output_csv_map_perc_frag_cov_mol_grouped, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_perc_frag_cov_mol_grouped}'")

    # top 10 most occurring unique fragments per molecule
    df_map_tmp = df_map.copy()
    df_map_tmp['frags'] = df_map_tmp['frags'].map(lambda x: json.loads(x.replace("'", '"')))
    df_map_tmp['frags_idf'] = df_map_tmp['frags'].map(lambda x: list(set([y.split(':')[0] for y in x])))
    df_map_tmp = df_map_tmp[['idm', 'frags_idf']]
    df_idf_counts = df_map_tmp.set_index(['idm'])['frags_idf'].apply(pd.Series).stack()
    df_idf_counts = df_idf_counts.reset_index()
    df_idf_counts.columns = ['idm', 'idh', 'idf']
    df_idf_counts = df_idf_counts.groupby('idf').count()[['idh']].rename({'idh': 'Count'}, axis=1).sort_values('Count', ascending=False)
    # df_top_10 = df_idf_counts.head(10)
    df_idf_counts.reset_index(inplace=True)
    df_idf_counts['Rank'] = df_idf_counts.index
    df_idf_counts['Rank'] = df_idf_counts['Rank'].map(lambda x: x + 1)
    # display the corresponding fragments
    df_frags = load.file(file_frags).drop(['status', 'task', "status_deglyco", "_Name", "inchikey"], axis=1)
    df_frags['idm'] = df_frags['idm'].astype(int)
    df_idf_counts['idf'] = df_idf_counts['idf'].astype(int)
    df_frags['idm'] = df_frags['idm'].astype(int)
    df_idf_counts = df_idf_counts.merge(df_frags, left_on='idf', right_on='idm')
    df_idf_counts.index = range(len(df_idf_counts.index))
    # total of fragments
    idf_counts_tot = df_idf_counts['Count'].sum()
    df_top10 = df_idf_counts.head(10)
    df_top10["smiles"] = df_top10['mol'].map(Chem.MolToSmiles)
    df_top10.drop(['mol', "idm"], axis=1, inplace=True)
    # percentage of the top 10
    df_top10['Perc'] = df_top10['Count'].map(lambda x: x / idf_counts_tot)
    df_top10['Perc_cum'] = df_top10['Count'].cumsum() / idf_counts_tot
    df_top10['Perc'] = df_top10['Perc'].map(lambda x: f"{x:.2%}")
    df_top10['Perc_cum'] = df_top10['Perc_cum'].map(lambda x: f"{x:.2%}")
    logger.info(f"MAP -- RESULTS -- TOP 10 MOST OCCURRING UNIQUE FRAGMENTS PER MOLECULE:\n\n{df_top10}\n")
    output_csv_map_top10_mol_u = output_csv_basename + "_map_top10_mol_u.csv"
    if compression is not None:
        output_csv_map_top10_mol_u += ".gz"
    df_top10.to_csv(output_csv_map_top10_mol_u, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_top10_mol_u}'")

    # top 10 most occurring fragments per molecule
    df_map_tmp = df_map.copy()
    df_map_tmp['frags'] = df_map_tmp['frags'].map(lambda x: json.loads(x.replace("'", '"')))
    groups_mol_map = df_map.groupby("idm")
    df_map_tmp = groups_mol_map.agg({'frags': 'sum'})
    df_map_tmp.reset_index(inplace=True)
    df_map_tmp["frags"] = df_map_tmp["frags"].map(lambda x: json.loads(x.replace("][", ",").replace("'", '"')))  # sum was supposed to concatenate lists, not convert them to str and then concatenate them
    df_map_tmp['frags'] = df_map_tmp['frags'].map(lambda x: list(set(x)))  # still 32:0, etc. so we can see when multiple fragments occur in one map
    df_idf_counts = df_map_tmp.set_index(['idm'])['frags'].apply(pd.Series).stack()
    df_idf_counts = df_idf_counts.reset_index()
    df_idf_counts.columns = ['idm', 'idh', 'idf']
    df_idf_counts["idf"] = df_idf_counts["idf"].map(lambda x: x.split(":")[0])
    df_idf_counts = df_idf_counts.groupby('idf').count()[['idh']].rename({'idh': 'Count'}, axis=1).sort_values('Count', ascending=False)
    df_idf_counts.reset_index(inplace=True)
    df_idf_counts['Rank'] = df_idf_counts.index
    df_idf_counts['Rank'] = df_idf_counts['Rank'].map(lambda x: x + 1)
    df_idf_counts['idf'] = df_idf_counts['idf'].astype(int)
    df_idf_counts = df_idf_counts.merge(df_frags, left_on='idf', right_on='idm')
    df_idf_counts.index = range(len(df_idf_counts.index))
    idf_counts_tot = df_idf_counts['Count'].sum()
    df_top10 = df_idf_counts.head(10)
    df_top10["smiles"] = df_top10['mol'].map(Chem.MolToSmiles)
    df_top10.drop(['mol', "idm"], axis=1, inplace=True)
    # percentage of the top 10
    df_top10['Perc'] = df_top10['Count'].map(lambda x: x / idf_counts_tot)
    df_top10['Perc_cum'] = df_top10['Count'].cumsum() / idf_counts_tot
    df_top10['Perc'] = df_top10['Perc'].map(lambda x: f"{x:.2%}")
    df_top10['Perc_cum'] = df_top10['Perc_cum'].map(lambda x: f"{x:.2%}")
    logger.info(f"MAP -- RESULTS -- TOP 10 MOST OCCURRING FRAGMENTS PER MOLECULE:\n\n{df_top10}\n")
    output_csv_map_top10_mol = output_csv_basename + "_map_top10_mol.csv"
    if compression is not None:
        output_csv_map_top10_mol += ".gz"
    df_top10.to_csv(output_csv_map_top10_mol, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_top10_mol}'")

    # combinations

    # initialization
    categories = ['fsp', 'fed', 'fbr', 'fli',
                  'cmo',
                  'cbs', 'cbe', 'cbb', 'cbl',
                  'cts', 'cte', 'ctb', 'ctl',
                  'cos', 'coe', 'cob', 'col',
                  ]
    df_map["comb"] = df_map["comb"].map(lambda x: json.loads(x.replace("'", '"')))

    # # all combs found in all fragment maps  # misleading?
    # ds_comb = []
    # for i in range(len(df_map.index)):
    #     d = dict(Counter(df_map.iloc[i]['comb']))
    #     ds_comb.append(d)
    # df_comb = pd.DataFrame(ds_comb, columns=categories).fillna(0)
    # df_comb = pd.DataFrame(df_comb.apply(lambda x: pd.to_numeric(x, downcast='integer')).sum()).rename({0: "Count"}, axis=1)
    # df_comb['Category'] = df_comb.index
    # df_comb['Perc'] = df_comb['Count'].map(lambda x: round(x/df_comb['Count'].sum(), 4)*100)
    # logger.info(f"MAP -- RESULTS -- FRAGMENT COMBINATIONS FOUND IN ALL FRAGMENT MAPS:\n\n{df_comb}\n")
    # output_csv_counts_sub = output_csv_basename + "_map_fragcomb.csv"
    # if compression is not None:
    #     output_csv_counts_sub += ".gz"
    # df_comb.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    # logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # all combs per molecule
    df_map_tmp = df_map[['idm', 'fmap_str', 'fmid']].copy()
    df_map_tmp["fmap_str"] = df_map_tmp["fmap_str"].map(lambda x: x.split("-"))
    groups_mol_map = df_map_tmp.groupby('idm')
    df_map_tmp = groups_mol_map.agg({'fmap_str': 'sum'})
    df_map_tmp.reset_index(inplace=True)
    # remove dupl. common parts between alternative fragment maps
    df_map_tmp["fmap_str"] = df_map_tmp["fmap_str"].map(lambda x: list(set(x)))
    df_map_tmp["comb"] = df_map_tmp["fmap_str"].map(lambda x: [y.split("[")[1].split("]")[0] for y in x])
    df_map_tmp.drop("fmap_str", axis=1, inplace=True)
    # count remaining combinations
    ds_comb = []
    for i in range(len(df_map_tmp.index)):
        d = dict(Counter(df_map_tmp.iloc[i]['comb']))
        ds_comb.append(d)
    df_comb = pd.DataFrame(ds_comb, columns=categories).fillna(0)
    df_comb = pd.DataFrame(df_comb.apply(lambda x: pd.to_numeric(x, downcast='integer')).sum()).rename({0: "Count"}, axis=1)
    df_comb['Category'] = df_comb.index
    num_tot = df_comb['Count'].sum()
    df_comb['Perc_FC'] = df_comb['Count'] /num_tot
    df_comb['Perc_FC'] = df_comb['Perc_FC'].map(lambda x: f"{x:.2%}")
    logger.info(f"MAP -- RESULTS -- FRAGMENT COMBINATIONS FOUND IN ALL FRAGMENT MAPS (tot={num_tot:,}):\n\n{df_comb}\n")
    output_csv_map_fc = output_csv_basename + "_map_fc.csv"
    if compression is not None:
        output_csv_map_fc += ".gz"
    df_comb.to_csv(output_csv_map_fc, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_map_fc}'")

    return {'map_frag_fm': output_csv_map_frag_fm,
            'map_frag_fm_u': output_csv_map_frag_fm_u,
            'map_frag_mol': output_csv_map_frag_mol,
            'map_frag_mol_u': output_csv_map_frag_mol_u,
            'map_perc_frag_cov_mol': output_csv_map_perc_frag_cov_mol,
            'map_perc_frag_cov_mol_grouped': output_csv_map_perc_frag_cov_mol_grouped,
            'map_top10_mol_u': output_csv_map_top10_mol_u,
            'map_top10_mol': output_csv_map_top10_mol,
            'map_fc': output_csv_map_fc,
            }

def get_result_pnp(WD: str, output_csv: str):

    logger.info("PLOT_RESULT_PNP")
    # go inside log subfolder in WD
    WD = WD + "data"
    logger.info(f"WD={WD}")
    WD = Path(WD)

    # iterate over the data files to count status
    chunks = list(WD.glob("*[0-9][0-9][0-9]_pnp.csv.gz"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_pnp_fm = 0
    num_non_pnp_fm = 0
    num_pnp_mol = 0
    num_non_pnp_mol = 0

    # begin to count
    for c in chunks:
        # fragment maps
        df = pd.read_csv(c, sep="|", compression="gzip")
        num_pnp_fm += len(df[df['pnp_fm']].index)
        num_non_pnp_fm += len(df[~df['pnp_fm']].index)
        # mols
        df.sort_values(["idm", "pnp_mol"], ascending=False, inplace=True)  # first member of a group is set to True
        df_mols = df.groupby("idm", sort=False).first()

        num_pnp_mol += len(df_mols[df_mols["pnp_mol"]].index)
        num_non_pnp_mol += len(df_mols[~df_mols["pnp_mol"]].index)

    # totals
    num_tot_fm = num_pnp_fm + num_non_pnp_fm
    logger.info(f"TOTAL NUMBER OF FRAGMENT MAPS: {num_tot_fm:,d}")
    num_tot_mol = num_pnp_mol + num_non_pnp_mol
    logger.info(f"TOTAL NUMBER OF MOLECULES: {num_tot_mol:,d}")

    # result
    df = pd.DataFrame({"PNP_fm": [num_pnp_fm], "Non-PNP_fm": [num_non_pnp_fm], "PNP_mol": [num_pnp_mol], "Non-PNP_mol": [num_non_pnp_mol]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_PNP'] = [x/num_tot_mol if 'mol' in y else x/num_tot_fm for x, y in zip(df["Count"], df['Category'])]
    df['Perc_PNP'] = df['Perc_PNP'].map(lambda x: f"{x:.2%}")
    logger.info(f"RESULTS:\n\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")
    return output_csv

if __name__ == '__main__':

    d0 = datetime.now()
    
    parser = argparse.ArgumentParser(description="Compute all required files for analyzing FCC results", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('wd', type=str, default='./', help="Working directory from the job to analyse (root folder, where the smk file is)")
    parser.add_argument('-f', '--frags', type=str, default=None, help="Fragment file used for substructure search")
    parser.add_argument('-d', '--dataset', type=str, default=None, help="Dataset name for using in the csv/png outputs in the figures folder.")
    parser.add_argument('-p', '--prefix', type=str, default=None, help="Prefix used for output files in the data/log folders.")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()


    # logging
    logger = utils._configure_logger(args.log)
    logger.info("PLEASE RUN THIS COMMAND FROM THE ROOT FOLDER OF THE NPFC DATA (fragments, natural, synthetic)")
    logger.info("RUNNING PLOT_PIPELINE_RESULTS")
    warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)  # if None is returned instead of a molecule, do not complain about mixed types
    pd.options.mode.chained_assignment = None  # disable pd.io.pytables.SettingWithCopyWarning
    pad = 40
    lg = RDLogger.logger()
    lg.setLevel(RDLogger.INFO)

    logger.info("LIBRARY VERSIONS:")
    logger.info("rdkit".ljust(pad) + f"{rdkit.__version__}")
    logger.info("pandas".ljust(pad) + f"{pd.__version__}")
    logger.info("npfc".ljust(pad) + f"{npfc.__version__}")

    if args.frags is None:
        logger.error("WARNING! FRAGS IS NOT DEFINED, SO NO ANALYSIS FURTHER THAN DUPL FILTERING")
    else:
        utils.check_arg_input_file(args.frags)

    # define WD
    p_wd = Path(args.wd)
    if not p_wd.exists():
        raise ValueError(f"ERROR! WD COULD NOT BE FOUND AT '{args.wd}'")
    wd = str(p_wd.resolve())
    if not wd.endswith("/"):
        wd += "/"
    p = Path(wd + "figures")
    if not p.is_dir():
        logger.warning(f"WARNING! OUTPUT SUBFOLDER MISSING, SO CREATING IT AT: {wd + 'figures'}")
        p.mkdir(parents=True)
    else:
        logger.info(f"OUTPUT SUBFOLDER LOCATED AT: {wd + 'figures'}")

    dataset = str(Path(wd).resolve(strict=True)).split('/')[-1]
    logger.info(f"DATASET='{dataset}'")

    d_dataset_labels = {'dnp': 'DNP', 'chembl': 'ChEMBL', 'cpa': 'CPA'}

    dataset_label = dataset
    for k, v in d_dataset_labels.items():
         dataset_label = dataset_label.replace(k, v)

    # general style for plotting
    sns.set(rc={'figure.figsize':(12, 12)})
    sns.set_style('whitegrid', {'axes.edgecolor': '0.2'})
    sns.set_context("paper", font_scale=2)

    # atribute color for individual plots
    if 'fragments' in args.wd:
        scenario = 'fragments'
        color = '#808080'
    elif 'natural' in args.wd:
        scenario = 'natural'
        color = '#2CA02C'
    elif 'synthetic' in args.wd:
        scenario = 'synthetic'
        color = '#204F6A'
    else:
        scenario = 'unknown'
        color = '#000000'
    logger.info(f"SCENARIO IS '{scenario}', attributing color='{color}'")

    # missing chunks

    logger.info("=" * pad)
    logger.info("CHECKING FOR MISSING CHUNKS".center(pad))
    logger.info("=" * pad)
    if check_chunks(wd):
        logger.info("ALL EXPECTED CHUNKS WERE FOUND!")

    # deglycosylation

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM DEGLYCOSYLATION".center(pad))
    logger.info("=" * pad)
    try:
        subfolder_deglyco = [str(x) for x in list(Path(wd).glob("data/*_deglyco"))][0] + "/"
        output_csv = get_result_deglyco(WD=subfolder_deglyco, output_csv=wd + f"figures/{dataset}_deglyco.csv")
        output_png = output_csv.replace('.csv', '.png')
        title = f"Deglycosylation of {dataset_label}"
        df = pd.read_csv(output_csv, sep='|')
        fig = save_barplot(df, output_png, 'Category', 'Count', title, color=color, perc_labels='Perc_status')
    except IndexError:
        logger.info("NO DEGLYCOSYLATION RESULTS TO CHECK")

    # standardization

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM STANDARDIZATION".center(pad))
    logger.info("=" * pad)
    try:
        subfolder_std = [str(x) for x in list(Path(wd).glob("data/*_std"))][0] + "/"
        output_csvs = get_result_std(WD=subfolder_std, output_csv=wd + f"figures/{dataset}_std.csv")
        keys = ['std', 'std_passed', 'std_filtered', 'std_error', 'std']  # I know this is very confusing, but I am in a hurry at the moment. I need the total count for detailed counts, but I also need to update the std using the detailed counts. This is a mess and I am sorry.
        tot_mols = None
        for i, k in enumerate(keys):
            # try:
            output_csv = output_csvs[k]
            output_png = output_csv.replace('.csv', '.png')
            title = f"Standardization of {dataset_label}"
            df_std = pd.read_csv(output_csv, sep='|')
            df_dupl = None
            df_load = None
            rotate_x = 60
            if i == 0:
                rotate_x = 0
                tot_mols = df_std['Count'].sum()  # so % are computed based on
                continue  # compute plot later only!
            elif i == 1:
                title += ' - Passed'
                total_passed = df_std['Count'].sum()
            elif i == 2:
                title += ' - Filtered'
                subfolder_uni = [str(x) for x in list(Path(wd).glob("data/*_uni"))][0] + "/"
                tmp_csv_uni = get_result_dupl(subfolder_uni, wd + f"figures/tmp_{dataset}_uni.csv")
                df_dupl = pd.read_csv(tmp_csv_uni, sep='|')
                df_dupl = df_dupl[df_dupl['Category'] == 'filtered']
                # concat both
                if len(df_dupl.index) > 0:
                    df_std = pd.concat([df_std, df_dupl])
                df_std.drop('Perc_status', axis=1, inplace=True)
                # sort by order of filter
                filters = ['empty', 'hac', 'molweight', 'nrings', 'medchem', 'timeout', 'duplicate']
                df_std.set_index('Category', inplace=True)
                df_std = df_std.reindex(filters)
                df_std.reset_index(inplace=True)
                df_std.fillna(0, inplace=True)
                df_std['Perc_status'] = df_std['Count'].map(lambda x: f"{x/tot_mols:.2%}")
                total_filtered = df_std['Count'].sum()
                df_std.to_csv(output_csv, sep='|', index=False)  # this is confusing!!! Adding the missing categories should be done right at the beginning? same for df_std_error
                logger.info(f"STD -- FINAL RESULTS FOR FILTERED:\n\n{df_std}\n")
            elif i == 3:
                title += ' - Error'
                subfolder_load = [str(x) for x in list(Path(wd).glob("data/*_load"))][0] + "/"
                tmp_csv_load = get_result_load(subfolder_load, wd + f"figures/tmp_{dataset}_load.csv")
                df_load = pd.read_csv(tmp_csv_load, sep='|')
                df_load = df_load[df_load['Category'] == 'error']
                df_load.replace({'error': 'load_mol'}, inplace=True)
                if len(df_load.index) > 0:
                    df_std = pd.concat([df_std, df_load])
                df_std.drop('Perc_status', axis=1, inplace=True)
                # sort by order of errors
                errors = ['load_mol', 'initiate_mol', 'disconnect_metal', 'sanitize', 'remove_isotopes', 'normalize', 'uncharge', 'canonicalize', 'remove_stereo']
                df_std.set_index('Category', inplace=True)
                df_std = df_std.reindex(errors)
                df_std.reset_index(inplace=True)
                df_std.fillna(0, inplace=True)
                df_std['Perc_status'] = df_std['Count'].map(lambda x: f"{x/tot_mols:.2%}")
                total_errors = df_std['Count'].sum()
                df_std.to_csv(output_csv, sep='|', index=False)
                logger.info(f"STD -- FINAL RESULTS FOR ERROR:\n\n{df_std}\n")
            elif i == 4:
                df_std = DataFrame({"passed": [total_passed], "filtered": [total_filtered], "error": [total_errors]}).T.rename({0: "Count"}, axis=1)
                df_std['Category'] = df_std.index
                df_std.reset_index(inplace=True, drop=True)
                num_tot = df_std['Count'].sum()
                if num_tot > 0:
                    df_std['Perc_status'] = df_std['Count'].map(lambda x: f"{x/num_tot:.2%}")
                else:
                    df_std['Perc_status'] = 'NA'
                logger.info(f"STD -- FINAL GENERAL RESULTS:\n\n{df_std}\n")

            if len(df_std) > 0:
                fig = save_barplot(df_std, output_png, 'Category', 'Count', title, color=color, perc_labels='Perc_status', rotate_x=rotate_x)
            else:
                logger.error("DF_STD IS EMPTY! NO FIGURE COULD BE PRINTED")
            # except ValueError:
            #     logger.error(f"Could not compute plot for key={k}")
    except IndexError:
        logger.error("NO STANDARDIZATION RESULTS TO CHECK")

    # mapping

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM MAPPING".center(pad))
    logger.info("=" * pad)
    output_csvs = None
    try:
        output_csvs = get_result_map(wd, wd + f"figures/{dataset}.csv", args.frags)
    except IndexError:
        logger.info("NO MAP RESULTS TO CHECK")

    if output_csvs is not None:
        for k in output_csvs.keys():
            output_csv = output_csvs[k]
            output_png = output_csv.replace('.csv', '.png')
            df = pd.read_csv(output_csv, sep='|')
            rotate_x = 0

            if k == 'map_frag_fm':
                plot_type = 'barplot'
                title = f"Number of Fragments in Fragment Maps in {dataset_label}"
                x = 'NumFrags'
                x_label = 'Number of Fragments'
                y = 'NumFragmentMaps'
                y_label = 'Number of Fragment Maps'
                perc_labels='Perc_FM'
                fig_size = (12, 12)
                continue # skip this plot as it is more confusing to explain than really useful... But I like to check how different results from fm and mols are to continue producing the csv

            elif k == 'map_frag_fm_u':
                plot_type = 'barplot'
                title = f"Number of Unique Fragments in Fragment Maps in {dataset_label}"
                x = 'NumUniqueFrags'
                x_label = 'Number of Fragments'
                y = 'NumFragmentMaps'
                y_label = 'Number of Fragment Maps'
                perc_labels='Perc_FM'
                fig_size = (12, 12)

            elif k == 'map_frag_mol':
                plot_type = 'barplot'
                title = f"Number of Fragments in Molecules in {dataset_label}"
                x = 'NumFrags'
                x_label = 'Number of Fragments'
                y = 'NumMols'
                y_label = 'Number of Molecules'
                perc_labels='Perc_Mols'
                fig_size = (12, 12)

            elif k == 'map_frag_mol_u':
                plot_type = 'barplot'
                title = f"Number of Unique Fragments in Molecules in {dataset_label}"
                x = 'NumUniqueFrags'
                x_label = 'Number of Fragments'
                y = 'NumMols'
                y_label = 'Number of Molecules'
                perc_labels='Perc_Mols'
                fig_size = (12, 12)

            elif k == 'map_perc_frag_cov_mol':
                plot_type = 'kdeplot'
                title = f"Distribution of Ratio of Fragments found in Molecules in {dataset_label}"
                x = 'Perc_Frag_Coverage_Per_Mol'
                x_label = 'Ratio Fragment/Molecule'
                y_label = 'Kernel Density Estimate'
                perc_labels='Perc_Mols'
                fig_size = (12, 12)

            elif k == 'map_top10_mol_u':
                plot_type = 'barplot'
                title = f"Top 10 Unique Fragments Found in {dataset_label}"
                x = 'idf'
                x_label = 'Fragment ID'
                y = 'Count'
                y_label = 'Occurrence'
                perc_labels='Perc'
                fig_size = (12, 12)

            elif k == 'map_top10_mol':
                plot_type = 'barplot'
                title = f"Top 10 Fragments Found in {dataset_label}"
                x = 'idf'
                x_label = 'Fragment ID'
                y = 'Count'
                y_label = 'Occurrence'
                perc_labels='Perc'
                fig_size = (12, 12)

            elif k == 'map_fc':
                plot_type = 'barplot'
                title = f"Fragment Combinations in {dataset_label}"
                x = 'Category'
                x_label = 'Fragment Combination Category'
                y = 'Count'
                y_label = 'Occurrence'
                perc_labels='Perc_FC'
                fig_size = (24, 12)
            else:
                continue
            logger.info(f"GENERATING PLOT FOR '{k}'")
            if plot_type == 'barplot':
                fig = save_barplot(df,
                                   output_png,
                                   x,
                                   y,
                                   title,
                                   x_label=x_label,
                                   y_label=y_label,
                                   color=color,
                                   perc_labels=perc_labels,
                                   rotate_x=rotate_x,
                                   fig_size=fig_size)
            elif plot_type == 'kdeplot':
                fig = save_kdeplot(df,
                                   output_png,
                                   x,
                                   title,
                                   x_label=x_label,
                                   y_label=y_label,
                                   color=color,
                                   fig_size=fig_size)
    # PNP

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM PNP".center(pad))
    logger.info("=" * pad)
    try:
        subfolder_pnp = [str(x) for x in list(Path(wd).glob("data/*_pnp"))][0] + "/"
        output_csv = get_result_pnp(subfolder_pnp, wd + f"figures/{dataset}_pnp.csv")
        output_png = output_csv.replace('.csv', '.png')
        df = pd.read_csv(output_csv, sep='|')
        df = df[df['Category'].str.contains('_mol')]
        plot_type = 'barplot'
        title = f"Number of Pseudo Natural Products (PNPs) in {dataset_label}"
        x = 'Category'
        x_label = 'Category'
        y = 'Count'
        y_label = 'Count'
        perc_labels='Perc_PNP'
        fig_size = (12, 12)
        rotate_x  = 0
        fig = save_barplot(df,
                           output_png,
                           x,
                           y,
                           title,
                           x_label=x_label,
                           y_label=y_label,
                           color=color,
                           perc_labels=perc_labels,
                           rotate_x=rotate_x,
                           fig_size=fig_size)
    except IndexError:
        logger.info("ERROR OR NO PNP RESULTS TO CHECK")

    # end
    d1 = datetime.now()
    logger.info("-- END OF REPORT")
    logger.info("COMPUTATIONAL TIME: TOTAL".ljust(pad * 2) + f"{d1-d0}")
