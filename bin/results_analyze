#!/usr/bin/env python

"""
Script plot_pipeline_results
==========================
This script is used for generating CSV files for plotting the results produced
by the FCC workflow. The root folder of the current job has to be specified
(Where the snakefile is located).

It will browse every subfolder and generate results in a plots subfolder in the
root folder.
"""

# standard
import warnings
import argparse
from collections import Counter
from pathlib import Path
from collections import OrderedDict
# data handling
import json
import pandas as pd
# chemoinformatics
import rdkit
from rdkit import Chem
from rdkit import RDLogger
# custom libraries
import npfc
from npfc import utils
from npfc import load


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def check_chunks(WD: str):
    """This functions will look up the expected amount of chunks produced by the
    "1_input" step and verify if all chunks are present in subsequent tasks.
    If not, it will raise a ValueError.

    Subfolders 0_raw and 1_input are excluded from the checks.

    :param WD: Working Directory of the pipeline to check (parent folder of the smk file)
    :return: True if no error was found, ValueError otherwise
    """
    # go inside data subfolder in WD
    WD = Path(WD + "data")

    # initialize the references for comparing
    chunks_ini = [str(s).split("_")[-1].split(".")[0] for s in list(WD.glob("1_input/data/*[0-9][0-9][0-9].sdf.gz"))]
    num_chunks_ini = len(chunks_ini)
    # define what subfolders are to be checked
    subfolders = [str(d).split("/")[-1] for d in WD.iterdir() if d.is_dir() if str(d).split("/")[-1] not in ("0_raw", "1_input")]

    # record errors in case of missing chunks
    errors = OrderedDict()
    for sf in subfolders:
        chunks_sf = [str(s).split("/")[-1] for s in list(WD.glob(str(sf) + "/data/*[0-9][0-9][0-9]*.gz"))]
        # currently no suffix is appended because of retrocompatibility issues
        if sf == "2_load":
            chunks_sf = [s.split("_")[-1] for s in chunks_sf]
        else:
            chunks_sf = [s.split("_")[-2] for s in chunks_sf]
        # check if the number of chunks match
        if len(chunks_sf) < num_chunks_ini:
            errors[sf] = [csf for csf in chunks_ini if csf not in chunks_sf]
    # raise a ValueError if any chunk is missing, describe which ones
    if errors:  # empty dicts are evaluated to False
        msg = "Error! Missing chunks found:\n"
        for sf in errors.keys():
            msg += f"{sf}".ljust(12) + f" ({len(errors[sf])}):\n".rjust(3) + ", ".join([c for c in errors[sf]]) + "\n"
        raise ValueError(msg)

    return True


def plot_result_deglyco(WD: str, output_csv: str):
    """This functions computes the CSV file for checking on the status of the deglycosylation.
    The WD is the subfolder of the deglycosylation (3_deglyco).
    Since the job is executed by a KNIME workflow, I used a log file to store the smiles alongside
    the status of the molecule. This is these files we are parsing like CSV.
    """
    logger.info("PLOT_RESULT_DEGLYCO")
    # go inside log subfolder in WD
    WD += "log"
    logger.info(f"WD={WD}")
    WD = Path(WD)

    # iterate over the log files to count status
    chunks = list(WD.glob("*[0-9][0-9][0-9]_deglyco.log"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_tot = 0
    num_unchanged = 0
    num_deglycosylated = 0
    num_failed = 0
    num_error = 0

    # count status
    for c in chunks:
        df = pd.read_csv(c, sep="|")
        num_tot += len(df.index)
        num_unchanged += len(df[df['status'] == 'unchanged'].index)
        num_deglycosylated += len(df[df['status'] == 'deglycosylated'].index)
        num_failed += len(df[df['status'] == 'failed'].index)
        num_error += len(df[df['status'] == 'error'].index)

    logger.info(f"TOTAL NUMBER OF MOLECULES: {num_tot:,d}")
    # create a dataframe with counts
    df = pd.DataFrame({"deglycosylated": [num_deglycosylated], "unchanged": [num_unchanged], "failed": [num_failed], 'error': [num_error]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_status'] = round(df['Count'] / num_tot, 3) * 100
    logger.info(f"RESULTS:\n\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")

    return df


def _parse_std_chunks(chunks):

    dfs = []
    for c in chunks:
        df = pd.read_csv(c, sep="|", compression="gzip").groupby("task").count()[['status']].rename({'status': 'Count'}, axis=1)
        if len(df.index) > 0:
            dfs.append(df)

    # if no case was found, return an empty dataframe
    if len(dfs) == 0:
        df = pd.DataFrame([], columns=["Count", "Category", "Perc_status"])
        return df

    # concatenate all dataframes and compute the sum of all counts
    df = pd.concat(dfs)
    df["Category"] = df.index  # I don't know how to group by index!
    df = df.groupby("Category").sum()
    df["Category"] = df.index.map(lambda x: x.replace('filter_', ''))
    num_tot = df["Count"].sum()
    df['Perc_status'] = round(df['Count'] / num_tot, 3) * 100

    return df


def plot_result_std(WD: str, output_csv: str):
    """This functions computes the CSV file for checking on the status of the standardization.
    The WD is the subfolder of the standardization (4_std) and it uses the log files for counting
    status.
    """
    logger.info("PLOT_RESULT_STD")

    # go inside log subfolder in WD
    WD_LOG = WD + "log"
    logger.info("GENERAL RESULTS")
    logger.info(f"WD={WD_LOG}")
    WD_LOG = Path(WD_LOG)

    # iterate over the log files to count status
    chunks = list(WD_LOG.glob("*[0-9][0-9][0-9]_std.log"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_tot = 0
    num_passed = 0
    num_filtered = 0
    num_error = 0

    for c in chunks:
        df = pd.read_csv(c, sep="@", header=None)  # char not found in the log file so we can extract all lines as one column
        passed, filtered, error = [int(x) for x in df[df[0].str.contains("COUNT")].iloc[0][0].split()[-3:]]
        num_passed += passed
        num_filtered += filtered
        num_error += error
        num_tot += passed + filtered + error

    df = pd.DataFrame({"passed": [num_passed], "filtered": [num_filtered], "error": [num_error]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_status'] = round(df['Count'] / num_tot, 3) * 100

    # display results
    logger.info(f"RESULTS:\n\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")

    # details

    # go inside data subfolder in WD
    WD_DATA = WD + "data"
    logger.info("DETAILED RESULTS")
    logger.info(f"WD={WD_DATA}")
    WD_DATA = Path(WD_DATA)

    # passed
    logger.info("PASSED")
    df_passed = pd.DataFrame([[num_passed, "standardized", 100.0]], columns=["Count", "Category", "Perc_status"])
    output_csv_passed = output_csv.split(".")[0] + "_passed.csv"
    if compression is not None:
        output_csv_passed += ".gz"
    df_passed.to_csv(output_csv_passed, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_passed}'")

    # filtered
    logger.info("FILTERED")
    chunks = list(WD_DATA.glob("*[0-9][0-9][0-9]_filtered.csv.gz"))
    chunks.sort()
    df_filtered = _parse_std_chunks(chunks)
    logger.info(f"RESULTS:\n\n{df_filtered}\n")

    output_csv_filtered = output_csv.split(".")[0] + "_filtered.csv"
    if compression is not None:
        output_csv_filtered += ".gz"
    df_filtered.to_csv(output_csv_filtered, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_filtered}'")

    # error
    logger.info("ERROR")
    chunks = list(WD_DATA.glob("*[0-9][0-9][0-9]_error.csv.gz"))
    chunks.sort()
    df_error = _parse_std_chunks(chunks)
    logger.info(f"RESULTS:\n\n{df_error}\n")

    output_csv_error = output_csv.split(".")[0] + "_error.csv"
    if compression is not None:
        output_csv_error += ".gz"
    df_error.to_csv(output_csv_error, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_error}'")

    return df


def plot_result_dupl(WD: str, output_csv: str):

    logger.info("PLOT_RESULT_DUPL")

    # go inside log subfolder in WD
    WD_LOG = WD + "log"
    logger.info("GENERAL RESULTS")
    logger.info(f"WD={WD_LOG}")
    WD_LOG = Path(WD_LOG)

    # iterate over the log files to count status
    chunks = list(WD_LOG.glob("*[0-9][0-9][0-9]_uni.log"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_tot = 0
    num_passed = 0
    num_filtered = 0

    for c in chunks:
        df = pd.read_csv(c, sep="@", header=None)  # char not found in the log file so we can extract all lines as one column
        passed, total = [int(x) for x in df[df[0].str.contains("REMAINING MOLECULES")].iloc[0][0].split("MOLECULES:")[1].split("/")]
        num_passed += passed
        num_filtered += total - passed
        num_tot += total

    df = pd.DataFrame({"passed": [num_passed], "filtered": [num_filtered]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_status'] = round(df['Count'] / num_tot, 3) * 100

    # display results
    logger.info(f"RESULTS:\n\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")


def plot_result_map(WD: str, output_csv: str, file_frags: str):
    logger.info("PLOT_RESULT_MAP")
    # go inside log subfolder in WD
    logger.info(f"WD={WD}")
    p = Path(WD)

    format, compression = utils.get_file_format(output_csv)
    output_csv_basename = output_csv.split(".csv")[0]
    logger.info(f"OUTPUTS BASENAME: {output_csv_basename}")

    # parse std results (num_mol_tot_std)
    logger.info("SYNTH -- COMPUTING TOTAL NUMBER OF MOLECULES BEFORE SUBSTRUCTURE SEARCH")
    chunks = list(p.glob("data/*_synth/data/*[0-9][0-9][0-9]_synth.csv.gz"))
    logger.info(f"SYNTH -- FOUND {len(chunks):,d} CHUNKS_SYNTH")
    # if no chunks found in synth (DNP, ect.), then use uni instead
    if len(chunks) == 0:
        chunks = list(p.glob("data/*_uni/data/*[0-9][0-9][0-9]_uni.csv.gz"))
        logger.info(f"UNI -- FOUND {len(chunks)} CHUNKS_STD. USING THEM INSTEAD OF CHUNKS_SYNTH")
    num_mol_tot_std = sum([len(pd.read_csv(x, sep='|', compression='gzip').index) for x in chunks])
    logger.info(f"UNI -- TOTAL NUMBER OF MOLECULES: {num_mol_tot_std:,d}")

    # parse sub results
    chunks = list(p.glob("data/*_sub/data/*[0-9][0-9][0-9]_sub.csv.gz"))
    chunks.sort()
    logger.info(f"SUB -- FOUND {len(chunks):,d} CHUNKS_SUB")
    df_sub = pd.concat([pd.read_csv(x, sep='|', compression='gzip') for x in chunks])
    num_tot_hit_sub = len(df_sub.index)
    logger.info(f"SUB -- TOTAL NUMBER OF SUBSTRUCTURE HITS: {num_tot_hit_sub:,d}")

    # regroup by molecule
    df_sub['count_hit_per_mol'] = df_sub.groupby(['idm', 'idf'])['aidxf'].transform(len)
    groups_mols_sub = df_sub[["idm", "idf", "count_hit_per_mol"]].groupby("idm")
    num_tot_mol_sub = len(groups_mols_sub)
    logger.info(f"SUB -- TOTAL NUMBER OF MOLECULES WITH >0 NATURAL FRAGMENTS: {num_tot_mol_sub:,d}")

    # entries with > 1 fragments
    num_mol_gt1_sub = len(groups_mols_sub.filter(lambda x: len(x) > 1).groupby("idm").first().index)
    logger.info(f"SUB -- NUMBER OF MOLECULES WITH >1 NATURAL FRAGMENTS: {num_mol_gt1_sub:,d}")

    # entries with 1 fragment
    num_mol_eq1_sub = len(groups_mols_sub.filter(lambda x: len(x) == 1).groupby("idm").first().index)
    logger.info(f"SUB -- NUMBER OF MOLECULES WITH 1 NATURAL FRAGMENT: {num_mol_eq1_sub:,d}")

    # missing entries (0 fragment)
    num_mol_eq0_sub = num_mol_tot_std - num_mol_eq1_sub - num_mol_gt1_sub
    logger.info(f"SUB -- NUMBER OF MOLECULES WITH 0 NATURAL FRAGMENT: {num_mol_eq0_sub:,d}")

    # # clear dupl
    # df_sub.drop_duplicates(["idm", "idf"], inplace=True)  # clear dupl only now because counts of mols before are based on the number of elements per group
    #
    # # counts
    # df_sub_counts = groups_mols_sub.sum().rename({"count_hit_per_mol": "NumSubstructHits"}, axis=1)
    # df_sub_counts = df_sub_counts.groupby("NumSubstructHits").count().rename({"idf": "NumMols"}, axis=1)
    # df_sub_counts = pd.concat([pd.DataFrame({"NumMols": [num_mol_eq0_sub]}, index=[0]), df_sub_counts], ignore_index=False)
    # df_sub_counts = df_sub_counts.reset_index().rename({"index": "NumSubstructHits"}, axis=1)
    # logger.info(f"SUB -- RESULTS -- SUBSTRUCTURE HITS:\n\n{df_sub_counts}\n")
    # output_csv_counts_sub = output_csv_basename + "_sub_counts.csv"
    # if compression is not None:
    #     output_csv_counts_sub += ".gz"
    # df_sub_counts.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    # logger.info(f"SUB -- SAVED RESULTS AT '{output_csv_counts_sub}'")
    #
    # # unique counts
    # df_sub_counts = groups_mols_sub.count().rename({"count_hit_per_mol": "NumUniqueSubstructHits"}, axis=1)
    # df_sub_counts = df_sub_counts.groupby("NumUniqueSubstructHits").count().rename({"idf": "NumMols"}, axis=1)
    # df_sub_counts = pd.concat([pd.DataFrame({"NumMols": [num_mol_eq0_sub]}, index=[0]), df_sub_counts], ignore_index=False)
    # df_sub_counts = df_sub_counts.reset_index().rename({"index": "NumUniqueSubstructHits"}, axis=1)
    # logger.info(f"SUB -- RESULTS -- UNIQUE SUBSTRUCTURE HITS:\n\n{df_sub_counts}\n")
    # output_csv_counts_unique_sub = output_csv_basename + "_sub_counts_unique.csv"
    # if compression is not None:
    #     output_csv_counts_unique_sub += ".gz"
    # df_sub_counts.to_csv(output_csv_counts_unique_sub, sep="|", compression=compression, index=False)
    # logger.info(f"SUB -- SAVED RESULTS AT '{output_csv_counts_unique_sub}'")

    # parse fcc results
    chunks = list(p.glob("data/*_fcc/data/*[0-9][0-9][0-9]_fcc.csv.gz"))
    chunks.sort()
    logger.info(f"FCC -- FOUND {len(chunks):,d} CHUNKS_FCC")
    df_fcc = pd.concat([pd.read_csv(x, sep='|', compression='gzip') for x in chunks])
    num_tot_comb_fcc = len(df_fcc.index)
    logger.info(f"FCC -- TOTAL NUMBER OF COMBINATIONS: {num_tot_comb_fcc:,d}")
    num_tot_mol_fcc = len(df_fcc.groupby('idm').first().index)
    logger.info(f"FCC -- TOTAL NUMBER OF MOLECULES: {num_tot_mol_fcc:,d}")

    # missing molecules (filtered because false positives)
    num_mol_fp_fcc = num_mol_gt1_sub - num_tot_mol_fcc
    logger.info(f"FCC -- NUMBER OF FILTERED MOLECULES BECAUSE OF FP: {num_mol_fp_fcc:,d}")

    # parse map results
    chunks = list(p.glob("data/*_map/data/*[0-9][0-9][0-9]_map.csv.gz"))
    chunks.sort()
    logger.info(f"MAP -- FOUND {len(chunks):,d} CHUNKS_MAP")
    df_map = pd.concat([pd.read_csv(x, sep="|", compression="gzip") for x in chunks])
    df_map.sort_values(["idm", "nfrags"], ascending=True, inplace=True)  # lowest number of frags per mol so first element of each group has the smallest fm
    num_tot_fm_map = len(df_map.index)
    logger.info(f"MAP -- TOTAL NUMBER OF FRAGMENT MAPS: {num_tot_fm_map:,d}")
    num_tot_mol_map = len(df_map.groupby('idm').first().index)
    logger.info(f"MAP -- TOTAL NUMBER OF MOLECULES: {num_tot_mol_map:,d}")

    # missing molecules (filtered because of overlaps and min/max frags)
    num_mol_fp_map = num_tot_mol_fcc - num_tot_mol_map
    logger.info(f"MAP -- NUMBER OF FILTERED MOLECULES BECAUSE OF FP: {num_mol_fp_map:,d}")

    # fragment counts per fm
    df_sub_counts = df_map.groupby('nfrags').count()[['idm']].rename({'idm': 'NumFragmentMaps'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags": "NumFrags"}, axis=1, inplace=True)
    logger.info(f"MAP -- RESULTS -- NUMBER OF FRAGMENTS FOUND PER FRAGMENT MAP:\n\n{df_sub_counts}\n")
    output_csv_counts_sub = output_csv_basename + "_map_numfragperfm_counts.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_sub_counts.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # unique fragment counts per fm
    df_sub_counts = df_map.groupby('nfrags_u').count()[['idm']].rename({'idm': 'NumFragmentMaps'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags_u": "NumUniqueFrags"}, axis=1, inplace=True)
    logger.info(f"MAP -- RESULTS -- NUMBER OF UNIQUE FRAGMENTS FOUND PER FRAGMENT MAP:\n\n{df_sub_counts}\n")
    output_csv_counts_sub = output_csv_basename + "_map_numuniquefragperfm_counts.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_sub_counts.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # smallest fm per mol
    df_map.sort_values(["idm", "nfrags"], inplace=True)  # ascending so first element of each idm group has the smallest nfrags
    df_map_mol = df_map.groupby('idm').first()
    df_map_mol.reset_index(inplace=True)
    df_map_mol.rename({"index": "idm"}, axis=True, inplace=True)

    # fragment counts per mol
    df_sub_counts = df_map_mol.groupby('nfrags').count()[['idm']].rename({'idm': 'NumMols'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags": "NumFrags"}, axis=1, inplace=True)
    logger.info(f"MAP -- RESULTS -- NUMBER OF FRAGMENTS FOUND PER MOLECULE:\n\n{df_sub_counts}\n")
    output_csv_counts_sub = output_csv_basename + "_map_numfragpermol_counts.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_sub_counts.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # unique fragment counts per mol
    df_sub_counts = df_map_mol.groupby('nfrags_u').count()[['idm']].rename({'idm': 'NumMols'}, axis=1)
    df_sub_counts.reset_index(inplace=True)
    df_sub_counts.rename({"nfrags_u": "NumUniqueFrags"}, axis=1, inplace=True)
    logger.info(f"MAP -- RESULTS -- NUMBER OF UNIQUE FRAGMENTS FOUND PER MOLECULE:\n\n{df_sub_counts}\n")
    output_csv_counts_sub = output_csv_basename + "_map_numuniquefragpermol_counts.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_sub_counts.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # fragment map coverage by fragments
    df_frag_fm_cov = df_map[["perc_mol_cov_frags", "idm"]]
    num_tot_fm_fcc_molcov = len(df_frag_fm_cov.index)
    df_frag_fm_cov_counts = df_frag_fm_cov.groupby("perc_mol_cov_frags").count()
    df_frag_fm_cov_counts.reset_index(inplace=True)
    df_frag_fm_cov_counts.rename({"perc_mol_cov_frags": "Perc_Frag_Coverage_Per_FragmentMap", "idm": "NumFragMaps"}, axis=1, inplace=True)
    df_frag_fm_cov_counts["PercFM"] = (df_frag_fm_cov_counts["NumFragMaps"] / num_tot_fm_fcc_molcov) * 100
    logger.info(f"MAP -- RESULTS -- PERC OF FRAGMENT COVERAGE PER FRAGMENT MAP:\n\n{df_frag_fm_cov_counts}\n")
    output_csv_counts_sub = output_csv_basename + "_map_perc_frag_cov_fm_counts.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_frag_fm_cov_counts.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # molecule coverage by fragments
    df_map["_colormap"] = df_map["_colormap"].map(utils.decode_object)
    df_map["aidxs"] = df_map["_colormap"].map(lambda x: [k for k in x.atoms.keys()])
    groups_mol_map = df_map[['idm', 'hac_mol', 'aidxs']].groupby("idm")
    num_tot_mol_fcc_molcov = len(groups_mol_map)
    logger.info(f"MAP -- FCC -- TOTAL NUMBER OF MOLS WHEN CHECKING COVERAGE PER MOLECULE: {num_tot_mol_fcc_molcov}")
    df_frag_mol_cov = groups_mol_map.agg({'aidxs': 'sum'})
    df_frag_mol_cov["hac_mol"] = groups_mol_map.first()['hac_mol']  # ok instead of joint because groups are in the same order
    df_frag_mol_cov["hac_frags"] = df_frag_mol_cov["aidxs"].map(lambda x: len(list(set(x))))
    df_frag_mol_cov.drop("aidxs", axis=1, inplace=True)
    df_frag_mol_cov["Perc_Frag_Coverage_Per_Mol"] = df_frag_mol_cov.apply(lambda x: round(x["hac_frags"] / x["hac_mol"], 2) * 100, axis=1)
    df_frag_mol_cov.drop("hac_frags", axis=1, inplace=True)
    df_frag_mol_cov.rename({"hac_mol": "NumMols"}, axis=1, inplace=True)
    df_frag_mol_cov_counts = df_frag_mol_cov.groupby("Perc_Frag_Coverage_Per_Mol").count()
    df_frag_mol_cov_counts["PercMols"] = (df_frag_mol_cov_counts["NumMols"] / num_tot_mol_fcc_molcov) * 100
    df_frag_mol_cov_counts.reset_index(inplace=True)
    logger.info(f"MAP -- RESULTS -- PERC OF FRAGMENT COVERAGE PER MOLECULE:\n\n{df_frag_mol_cov_counts}\n")
    output_csv_counts_sub = output_csv_basename + "_map_perc_frag_cov_mol_counts.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_frag_mol_cov_counts.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # top 10 most occurring unique fragments per molecule
    df_map_tmp = df_map.copy()
    df_map_tmp['frags'] = df_map_tmp['frags'].map(lambda x: json.loads(x.replace("'", '"')))
    df_map_tmp['frags_idf'] = df_map_tmp['frags'].map(lambda x: list(set([y.split(':')[0] for y in x])))
    df_map_tmp = df_map_tmp[['idm', 'frags_idf']]
    df_idf_counts = df_map_tmp.set_index(['idm'])['frags_idf'].apply(pd.Series).stack()
    df_idf_counts = df_idf_counts.reset_index()
    df_idf_counts.columns = ['idm', 'idh', 'idf']
    df_idf_counts = df_idf_counts.groupby('idf').count()[['idh']].rename({'idh': 'Count'}, axis=1).sort_values('Count', ascending=False)
    # df_top_10 = df_idf_counts.head(10)
    df_idf_counts.reset_index(inplace=True)
    df_idf_counts['Rank'] = df_idf_counts.index
    df_idf_counts['Rank'] = df_idf_counts['Rank'].map(lambda x: x + 1)
    # display the corresponding fragments
    df_frags = load.file(file_frags).drop(['status', 'task', "status_deglyco", "_Name", "inchikey"], axis=1)
    df_frags['idm'] = df_frags['idm'].astype(int)
    df_idf_counts['idf'] = df_idf_counts['idf'].astype(int)
    df_frags['idm'] = df_frags['idm'].astype(int)
    df_idf_counts = df_idf_counts.merge(df_frags, left_on='idf', right_on='idm')
    df_idf_counts.index = range(len(df_idf_counts.index))
    # total of fragments
    idf_counts_tot = df_idf_counts['Count'].sum()
    df_top10 = df_idf_counts.head(10)
    df_top10["smiles"] = df_top10['mol'].map(Chem.MolToSmiles)
    df_top10.drop(['mol', "idm"], axis=1, inplace=True)
    # percentage of the top 10
    df_top10['Perc'] = df_top10['Count'].map(lambda x: round(x / idf_counts_tot, 4) * 100)
    df_top10['Perc_cum'] = 100 * round(df_top10['Count'].cumsum() / idf_counts_tot, 4)
    logger.info(f"MAP -- RESULTS -- TOP 10 MOST OCCURRING UNIQUE FRAGMENTS PER MOLECULE:\n\n{df_top10}\n")
    output_csv_counts_sub = output_csv_basename + "_map_top10_unique.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_top10.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # top 10 most occurring fragments per molecule
    df_map_tmp = df_map.copy()
    df_map_tmp['frags'] = df_map_tmp['frags'].map(lambda x: json.loads(x.replace("'", '"')))
    groups_mol_map = df_map.groupby("idm")
    df_map_tmp = groups_mol_map.agg({'frags': 'sum'})
    df_map_tmp.reset_index(inplace=True)
    df_map_tmp["frags"] = df_map_tmp["frags"].map(lambda x: json.loads(x.replace("][", ",").replace("'", '"')))  # sum was supposed to concatenate lists, not convert them to str and then concatenate them
    df_map_tmp['frags'] = df_map_tmp['frags'].map(lambda x: list(set(x)))  # still 32:0, etc. so we can see when multiple fragments occur in one map
    df_idf_counts = df_map_tmp.set_index(['idm'])['frags'].apply(pd.Series).stack()
    df_idf_counts = df_idf_counts.reset_index()
    df_idf_counts.columns = ['idm', 'idh', 'idf']
    df_idf_counts["idf"] = df_idf_counts["idf"].map(lambda x: x.split(":")[0])
    df_idf_counts = df_idf_counts.groupby('idf').count()[['idh']].rename({'idh': 'Count'}, axis=1).sort_values('Count', ascending=False)
    df_idf_counts.reset_index(inplace=True)
    df_idf_counts['Rank'] = df_idf_counts.index
    df_idf_counts['Rank'] = df_idf_counts['Rank'].map(lambda x: x + 1)
    df_idf_counts['idf'] = df_idf_counts['idf'].astype(int)
    df_idf_counts = df_idf_counts.merge(df_frags, left_on='idf', right_on='idm')
    df_idf_counts.index = range(len(df_idf_counts.index))
    idf_counts_tot = df_idf_counts['Count'].sum()
    df_top10 = df_idf_counts.head(10)
    df_top10["smiles"] = df_top10['mol'].map(Chem.MolToSmiles)
    df_top10.drop(['mol', "idm"], axis=1, inplace=True)
    # percentage of the top 10
    df_top10['Perc'] = df_top10['Count'].map(lambda x: round(x / idf_counts_tot, 4) * 100)
    df_top10['Perc_cum'] = 100 * round(df_top10['Count'].cumsum() / idf_counts_tot, 4)
    logger.info(f"MAP -- RESULTS -- TOP 10 MOST OCCURRING FRAGMENTS PER MOLECULE:\n\n{df_top10}\n")
    output_csv_counts_sub = output_csv_basename + "_map_top10_mol.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_top10.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # combinations

    # initialization
    categories = ['fsp', 'fed', 'fbr', 'fot',
                  'cmo',
                  'cbs', 'cbe', 'cbb', 'cbo',
                  'cts', 'cte', 'ctb', 'cto',
                  'cos', 'coe', 'cob', 'coo',
                  ]
    df_map["comb"] = df_map["comb"].map(lambda x: json.loads(x.replace("'", '"')))

    # # all combs found in all fragment maps  # misleading?
    # ds_comb = []
    # for i in range(len(df_map.index)):
    #     d = dict(Counter(df_map.iloc[i]['comb']))
    #     ds_comb.append(d)
    # df_comb = pd.DataFrame(ds_comb, columns=categories).fillna(0)
    # df_comb = pd.DataFrame(df_comb.apply(lambda x: pd.to_numeric(x, downcast='integer')).sum()).rename({0: "Count"}, axis=1)
    # df_comb['Category'] = df_comb.index
    # df_comb['Perc'] = df_comb['Count'].map(lambda x: round(x/df_comb['Count'].sum(), 4)*100)
    # logger.info(f"MAP -- RESULTS -- FRAGMENT COMBINATIONS FOUND IN ALL FRAGMENT MAPS:\n\n{df_comb}\n")
    # output_csv_counts_sub = output_csv_basename + "_map_fragcomb.csv"
    # if compression is not None:
    #     output_csv_counts_sub += ".gz"
    # df_comb.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    # logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")

    # all combs per molecule
    df_map_tmp = df_map[['idm', 'map_str', 'fmid']].copy()
    df_map_tmp["map_str"] = df_map_tmp["map_str"].map(lambda x: x.split("-"))
    groups_mol_map = df_map_tmp.groupby('idm')
    df_map_tmp = groups_mol_map.agg({'map_str': 'sum'})
    df_map_tmp.reset_index(inplace=True)
    # remove dupl. common parts between alternative fragment maps
    df_map_tmp["map_str"] = df_map_tmp["map_str"].map(lambda x: list(set(x)))
    df_map_tmp["comb"] = df_map_tmp["map_str"].map(lambda x: [y.split("[")[1].split("]")[0] for y in x])
    df_map_tmp.drop("map_str", axis=1, inplace=True)
    # count remaining combinations
    ds_comb = []
    for i in range(len(df_map_tmp.index)):
        d = dict(Counter(df_map_tmp.iloc[i]['comb']))
        ds_comb.append(d)
    df_comb = pd.DataFrame(ds_comb, columns=categories).fillna(0)
    df_comb = pd.DataFrame(df_comb.apply(lambda x: pd.to_numeric(x, downcast='integer')).sum()).rename({0: "Count"}, axis=1)
    df_comb['Category'] = df_comb.index
    df_comb['Perc'] = df_comb['Count'].map(lambda x: round(x/df_comb['Count'].sum(), 4)*100)
    logger.info(f"MAP -- RESULTS -- FRAGMENT COMBINATIONS FOUND IN ALL FRAGMENT MAPS:\n\n{df_comb}\n")
    output_csv_counts_sub = output_csv_basename + "_map_fragcomb.csv"
    if compression is not None:
        output_csv_counts_sub += ".gz"
    df_comb.to_csv(output_csv_counts_sub, sep="|", compression=compression, index=False)
    logger.info(f"MAP -- SAVED RESULTS AT '{output_csv_counts_sub}'")


def plot_result_pnp(WD: str, output_csv: str):

    logger.info("PLOT_RESULT_PNP")
    # go inside log subfolder in WD
    WD = WD + "data"
    logger.info(f"WD={WD}")
    WD = Path(WD)

    # iterate over the data files to count status
    chunks = list(WD.glob("*[0-9][0-9][0-9]_pnp.csv.gz"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks):,d} CHUNKS")

    # initiate counts
    num_pnp_fm = 0
    num_non_pnp_fm = 0
    num_pnp_mol = 0
    num_non_pnp_mol = 0

    # begin to count
    for c in chunks:
        # fragment maps
        df = pd.read_csv(c, sep="|", compression="gzip")
        num_pnp_fm += len(df[df['pnp_fm']].index)
        num_non_pnp_fm += len(df[~df['pnp_fm']].index)
        # mols
        df.sort_values(["idm", "pnp_mol"], ascending=False, inplace=True)  # first member of a group is set to True
        df_mols = df.groupby("idm", sort=False).first()  #

        num_pnp_mol += len(df_mols[df_mols["pnp_mol"]].index)
        num_non_pnp_mol += len(df_mols[~df_mols["pnp_mol"]].index)

    # totals
    num_tot_fm = num_pnp_fm + num_non_pnp_fm
    logger.info(f"TOTAL NUMBER OF FRAGMENT MAPS: {num_tot_fm:,d}")
    num_tot_mol = num_pnp_mol + num_non_pnp_mol
    logger.info(f"TOTAL NUMBER OF MOLECULES: {num_tot_mol:,d}")

    # result
    df = pd.DataFrame({"PNP_fm": [num_pnp_fm], "Non-PNP_fm": [num_non_pnp_fm], "PNP_mol": [num_pnp_mol], "Non-PNP_mol": [num_non_pnp_mol]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc'] = [round(x/num_tot_mol, 3) * 100 if 'mol' in y else round(x/num_tot_fm, 3) * 100 for x, y in zip(df["Count"], df['Category'])]
    logger.info(f"RESULTS:\n\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="Compute all required files for analyzing FCC results", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('wd', type=str, default='./', help="Working directory from the job to analyse (root folder, where the smk file is)")
    parser.add_argument('-f', '--frags', type=str, default='None', help="Fragment file used for substructure search")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()

    # logging
    logger = utils._configure_logger(args.log)
    logger.info("RUNNING PLOT_PIPELINE_RESULTS")
    warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)  # if None is returned instead of a molecule, do not complain about mixed types
    pd.options.mode.chained_assignment = None  # disable pd.io.pytables.SettingWithCopyWarning
    pad = 40
    lg = RDLogger.logger()
    lg.setLevel(RDLogger.INFO)

    logger.info("LIBRARY VERSIONS:")
    logger.info("rdkit".ljust(pad) + f"{rdkit.__version__}")
    logger.info("pandas".ljust(pad) + f"{pd.__version__}")
    logger.info("npfc".ljust(pad) + f"{npfc.__version__}")

    if args.frags is None:
        logger.error("WARNING! FRAGS IS NOT DEFINED, SO NO ANALYSIS FURTHER THAN DUPL FILTERING")
    else:
        utils.check_arg_input_file(args.frags)

    # define WD
    if args.wd.endswith("/"):
        wd = args.wd
    else:
        wd = args.wd + "/"
    p = Path(wd + "figures")
    if not p.is_dir():
        logger.warning(f"WARNING! OUTPUT SUBFOLDER MISSING, SO CREATING IT AT: {wd + 'figures'}")
        p.mkdir(parents=True)
    else:
        logger.info(f"OUTPUT SUBFOLDER LOCATED AT: {wd + 'figures'}")

    dataset = wd.split("/")[-2]
    logger.info(f"DATASET='{dataset}'")

    # missing chunks

    logger.info("=" * pad)
    logger.info("CHECKING FOR MISSING CHUNKS".center(pad))
    logger.info("=" * pad)
    if check_chunks(wd):
        logger.info("ALL EXPECTED CHUNKS WERE FOUND!")

    # deglycosylation

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM DEGLYCOSYLATION".center(pad))
    logger.info("=" * pad)
    subfolder_deglyco = [str(x) for x in list(Path(wd).glob("data/*_deglyco"))][0] + "/"
    plot_result_deglyco(subfolder_deglyco, wd + f"figures/{dataset}_deglyco.csv")

    # standardization

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM STANDARDIZATION".center(pad))
    logger.info("=" * pad)
    try:
        subfolder_std = [str(x) for x in list(Path(wd).glob("data/*_std"))][0] + "/"
        plot_result_std(subfolder_std, wd + f"figures/{dataset}_std.csv")
    except IndexError:
        logger.info("NO STANDARDIZATION RESULTS TO CHECK")

    # duplicates

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM DUPLICATE FILTERING".center(pad))
    logger.info("=" * pad)
    subfolder_uni = [str(x) for x in list(Path(wd).glob("data/*_uni"))][0] + "/"
    plot_result_dupl(subfolder_uni, wd + f"figures/{dataset}_uni.csv")

    # mapping

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM MAPPING".center(pad))
    logger.info("=" * pad)
    try:
        plot_result_map(wd, wd + f"figures/{dataset}.csv", args.frags)
    except IndexError:
        logger.info("NO MAP RESULTS TO CHECK")

    # PNP

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM PNP".center(pad))
    logger.info("=" * pad)
    try:
        subfolder_pnp = [str(x) for x in list(Path(wd).glob("data/*_pnp"))][0] + "/"
        plot_result_pnp(subfolder_pnp, wd + f"figures/{dataset}_pnp.csv")
    except IndexError:
        logger.info("NO PNP RESULTS TO CHECK")
