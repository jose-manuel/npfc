#!/usr/bin/env python

"""
Script fct_target
=============================
This script generates the target nodes file for a given input chunk.

It will certainly be important to make sure not to create duplicate targets when
assembling the network.
"""
import argparse
from datetime import datetime
import sys
# data
import pandas as pd
# chemoinformatics
import rdkit
# dev
import npfc
from npfc import load
from npfc import save
from npfc import utils


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def main():

    # init
    d0 = datetime.now()
    description = """Script for generating the target nodes table.
    Rather than directly using the raw table, I prefer to filter it with the
    molecule ids found in the provided chunk. This will hopefully prevents from
    having a lot of single nodes in the network.

    Example:

        >>> fct_target fct/data/03_synthetic/raw/target.csv.gz fct/data/target_assay/data/target.csv.gz fct/data/03_synthetic/target/target.csv.gz

    """

    # parameters CLI
    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('input_target_table', type=str, default=None, help="The raw input target table for the corresponding dataset.")
    parser.add_argument('input_target_assay_table', type=str, default=None, help="The input molecule table.")
    parser.add_argument('output_table', type=str, default=None, help="Output file with the target - assay relationships.")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()

    # logging
    logger = utils._configure_logger(args.log)
    logger.info("RUNNING FCT_TARGET_ASSAY")
    pad = 40

    # parse arguments
    utils.check_arg_input_file(args.input_target_table)
    utils.check_arg_input_file(args.input_target_assay_table)
    utils.check_arg_output_file(args.output_table)

    # display infos

    # versions
    logger.info("LIBRARY VERSIONS:")
    logger.info("rdkit".ljust(pad) + f"{rdkit.__version__}")
    logger.info("pandas".ljust(pad) + f"{pd.__version__}")
    logger.info("npfc".ljust(pad) + f"{npfc.__version__}")
    # arguments
    logger.info("ARGUMENTS:")
    logger.info("INPUT_TARGET_TABLE".ljust(pad) + f"{args.input_target_table}")
    logger.info("INPUT_TARGET_ASSAY_TABLE".ljust(pad) + f"{args.input_target_assay_table}")
    logger.info("OUTPUT_TARGET_TABLE".ljust(pad) + f"{args.output_table}")
    # log
    logger.info("LOG".ljust(pad) + f"{args.log}")

    # begin

    logger.info("BEGIN")
    d1 = datetime.now()

    # load inputs
    logger.info("LOADING INPUT FILES")
    df_target_raw = load.file(args.input_target_table)
    logger.info(f"LOADED {len(df_target_raw)} TARGET NODES")
    df_target_assay = load.file(args.input_target_assay_table)[['target_id']]
    logger.info(f"LOADED {len(df_target_assay):,} TARGET-ASSAY RELATIONSHIPS")
    d2 = datetime.now()

    # process data
    df_target = df_target_raw[df_target_raw['target_id'].isin(df_target_assay['target_id'])]
    logger.info(f"REMAINING NUMBER OF TARGET NODES: {len(df_target)}")
    d3 = datetime.now()

    # save file
    logger.info("SAVING OUTPUT")
    save.file(df_target, args.output_table)
    d4 = datetime.now()

    # end

    logger.info("SUMMARY")
    logger.info("COMPUTATIONAL TIME: CONFIGURING JOB".ljust(pad * 2) + f"{d1-d0}")
    logger.info("COMPUTATIONAL TIME: LOADING INPUTS".ljust(pad * 2) + f"{d2-d1}")
    logger.info("COMPUTATIONAL TIME: PROCESSING DATA".ljust(pad * 2) + f"{d3-d2}")
    logger.info("COMPUTATIONAL TIME: SAVING OUTPUT".ljust(pad * 2) + f"{d4-d3}")
    logger.info("COMPUTATIONAL TIME: TOTAL".ljust(pad * 2) + f"{d4-d0}")
    logger.info("END")
    sys.exit(0)


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ MAIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


if __name__ == '__main__':
    main()
    sys.exit(0)
