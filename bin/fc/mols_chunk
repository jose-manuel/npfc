#!/usr/bin/env python

"""
Script mols_chunk
==================
This script is used for splitting up a file (SDF, CSV, HDF) into one or several chunks.
"""

# standard
import sys
import warnings
from pathlib import Path
from datetime import datetime
import logging
import argparse
import random
import time
# data
import pandas as pd
# chemoinformatics
import rdkit
from rdkit import RDLogger
# dev
import npfc
from npfc import load
from npfc import save
from npfc import utils


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def main():

    # init
    d0 = datetime.now()
    description = """mols_chunk
    A script for splitting up an input file into one or several chunks.
    In case the input file is compressed (.gzip), the output files will be too.

    Chunks will be appended with a suffix of syntax xxx_001.yyy, xxx_002.yyy, etc.

    The number of leading zeros is currently hard-coded to 3, so larger input files 
    with suffix numbers over 999 will not be sorted accordingly (only a display issue).

    A hard-coded file will be generated in the snakemake workflows "natural" and "synthetic",
    which indicates how many mols are present in the input mols file as well as how many chunks
    should be computed. This file will be located at:
    
    f"{config['WD']}/data/00_raw/data/{config['prefix']}_chunk_size.json"

    """

    # parameters CLI

    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('input_mols', type=str, default=None, help="Input file.")
    parser.add_argument('output_dir', type=str, default=None, help="Template output file path chunks should follow. Chunks have the same file name as the template, with an added sequential suffix (i.e. file.sdf.gz becomes file_001.sdf.gz, file_001.sdf.gz, etc.).")
    parser.add_argument('-c', '--chunk_size', type=int, default=5000, help="Maximum number of molecules per chunk.")
    parser.add_argument('-p', '--prefix', type=str, default=None, help="Prefix to use as chunk base file name. If left to None, input SDF file name is used.")
    parser.add_argument('-s', '--shuffle', action='store_true', help="Shuffle molecules, requires to load whole input file into memory.")
    parser.add_argument('--random-seed', type=int, default=None, help="Random seed for reproducing shuffle.")
    parser.add_argument('--csv-sep', type=str, default='|', help="Separator to use in case the input file is a CSV.")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()
    d0 = datetime.now()


    # logging
    logger = utils._configure_logger(args.log, reset_handlers=False)
    logger.info("RUNNING MOLS_CHUNK")
    pad = 40
    d0 = datetime.now()


    # parse arguments

    # check on args values not already checked by argparse
    utils.check_arg_input_file(args.input_mols)
    utils.check_arg_output_dir(args.output_dir)

    # chunk_size
    if args.chunk_size is not None and args.chunk_size < 1:
        raise ValueError("ERROR! PARAMETER CHUNK_SIZE CANNOT BE LOWER THAN 1! (%s)", args.chunk_size)
    
    # IO infos
    format, compression = utils.get_file_format(args.input_mols)
    output_dir = Path(args.output_dir)
    if args.prefix is None:
        output_mols_template = output_dir / Path(args.input_mols).name
    else:
        output_mols_template = output_dir / args.prefix


    # display infos

    # versions
    logger.info("LIBRARY VERSIONS:")
    logger.info("rdkit".ljust(pad) + f"{rdkit.__version__}")
    logger.info("pandas".ljust(pad) + f"{pd.__version__}")
    logger.info("npfc".ljust(pad) + f"{npfc.__version__}")
    # arguments
    # replace many vars with defined names for simplifying the whole pipeline, 
    # these variables might be added back when the structure of npfc does not change anymore
    logger.info("ARGUMENTS:")
    logger.info("INPUT_MOLS".ljust(pad) + f"{args.input_mols}")
    logger.info("OUTPUT_DIR".ljust(pad) + f"{args.output_dir}")
    logger.info("PREFIX".ljust(pad) + f"{args.prefix}")
    logger.info("OUTPUT_MOLS_TEMPLATE".ljust(pad) + f"{output_mols_template}")
    logger.info("FORMAT".ljust(pad) + f"{format}")
    logger.info("COMPRESSION".ljust(pad) + f"{compression}")
    logger.info("chunk_size".ljust(pad) + f"{args.chunk_size}")
    logger.info("SHUFFLE".ljust(pad) + f"{args.shuffle}")
    if args.shuffle:
        logger.info("RANDOM_SEED".ljust(pad) + f"{args.random_seed}")
    if format == 'CSV':
        logger.info("CSV_SEP".ljust(pad) + f"{args.csv_sep}")
    logger.info("LOG".ljust(pad) + f"{args.log}")
    d1 = datetime.now()


    # begin
    logger.info("BEGIN")

    # specific chunking for SDF to SDF without loading the whole file in memory
    if format == 'SDF':
        d2 = datetime.now()
        output_chunks = save.chunk_sdf(input_sdf=args.input_mols, output_dir=args.output_dir, chunk_size=args.chunk_size, prefix=args.prefix)
        d3 = datetime.now()
    else:
        if format == 'HDF':
            df_mols = pd.read_hdf(args.input_mols)
        elif format == 'CSV':
            df_mols = pd.read_csv(args.input_mols, sep=args.csv_sep)  # do not do any kind of parsing at all, just load into memory and save asap
        else:
            raise ValueError(f"ERROR! UNSUPPORTED FORMAT '{format}'")
        d2 = datetime.now()
        output_chunks = save.chunk(df_mols,
                                   output_mols_template,
                                   chunk_size=args.chunk_size,
                                   shuffle=args.shuffle,
                                   random_seed=args.random_seed,
                                   csv_sep=args.csv_sep,
                                   encode=False,
                                   )
        d3 = datetime.now()
    
    # display infos
    msg = "OUTPUT_CHUNKS:\n\n"
    msg += "OUTPUT_CHUNK".ljust(pad * 3) + 'NUM_MOLS'.rjust(10) + '\n'
    for output_chunk in output_chunks:
        msg += str(output_chunk[0]).ljust(pad * 3) + f"{output_chunk[1]:,}".rjust(10) + '\n'
    msg += '\n'
    logger.info(msg)

    # end
    logger.info("SUMMARY")
    logger.info("COMPUTATIONAL TIME: CONFIGURING JOB".ljust(pad * 2) + f"{d1-d0}")
    logger.info("COMPUTATIONAL TIME: LOADING MOLECULES".ljust(pad * 2) + f"{d2-d1}")
    logger.info("COMPUTATIONAL TIME: CHUNKING MOLECULES".ljust(pad * 2) + f"{d3-d2}")
    logger.info("COMPUTATIONAL TIME: TOTAL".ljust(pad * 2) + f"{d3-d0}")
    logger.info("END")
    sys.exit(0)


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ MAIN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


if __name__ == '__main__':
    main()
