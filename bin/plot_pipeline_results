#!/usr/bin/env python

"""
Script plot_pipeline_results
==========================
This script is used for generating CSV files for plotting the results produced
by the FCC workflow. The root folder of the current job has to be specified
(Where the snakefile is located).

It will browse every subfolder and generate results in a plots subfolder in the
root folder.
"""

# standard
import sys
import warnings
from datetime import datetime
import logging
import argparse
import subprocess
from pathlib import Path
from collections import OrderedDict
# data handling
import pandas as pd
import numpy as np
# chemoinformatics
import rdkit
from rdkit import RDLogger
# custom libraries
import npfc
from npfc import utils


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def check_chunks(WD: str):
    """This functions will look up the expected amount of chunks produced by the
    "1_input" step and verify if all chunks are present in subsequent tasks.
    If not, it will raise a ValueError.

    Subfolders 0_raw and 1_input are excluded from the checks.

    :param WD: Working Directory of the pipeline to check (parent folder of the smk file)
    :return: True if no error was found, ValueError otherwise
    """
    # go inside data subfolder in WD
    WD = Path(WD + "data")

    # initialize the references for comparing
    chunks_ini = [str(s).split("_")[-1].split(".")[0] for s in list(WD.glob("1_input/data/*[0-9][0-9][0-9].sdf.gz"))]
    num_chunks_ini = len(chunks_ini)
    # define what subfolders are to be checked
    subfolders = [str(d).split("/")[-1] for d in WD.iterdir() if d.is_dir() if str(d).split("/")[-1] not in ("0_raw", "1_input")]

    # record errors in case of missing chunks
    errors = OrderedDict()
    for sf in subfolders:
        chunks_sf = [str(s).split("/")[-1] for s in list(WD.glob(str(sf) + "/data/*[0-9][0-9][0-9]*.gz"))]
        # currently no suffix is appended because of retrocompatibility issues
        if sf == "2_load":
            chunks_sf = [s.split("_")[-1] for s in chunks_sf]
        else:
            chunks_sf = [s.split("_")[-2] for s in chunks_sf]
        # check if the number of chunks match
        if len(chunks_sf) < num_chunks_ini:
            errors[sf] = [csf for csf in chunks_ini if csf not in chunks_sf]
    # raise a ValueError if any chunk is missing, describe which ones
    if errors:  # empty dicts are evaluated to False
        msg = "Error! Missing chunks found:\n"
        for sf in errors.keys():
            msg += f"{sf}".ljust(12) + f" ({len(errors[sf])}):\n".rjust(3) + ", ".join([c for c in errors[sf]]) + "\n"
        raise ValueError(msg)

    return True


def plot_result_deglyco(WD: str, output_csv: str):
    """This functions computes the CSV file for checking on the status of the deglycosylation.
    The WD is the subfolder of the deglycosylation (3_deglyco).
    Since the job is executed by a KNIME workflow, I used a log file to store the smiles alongside
    the status of the molecule. This is these files we are parsing like CSV.
    """
    logger.info("PLOT_RESULT_DEGLYCO")
    # go inside log subfolder in WD
    WD += "log"
    logger.info(f"WD={WD}")
    WD = Path(WD)

    # iterate over the log files to count status
    chunks = list(WD.glob("*[0-9][0-9][0-9]_deglyco.log"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks)} CHUNKS")

    # initiate counts
    num_tot = 0
    num_unchanged = 0
    num_deglycosylated = 0
    num_failed = 0
    num_error = 0

    # count status
    for c in chunks:
        df = pd.read_csv(c, sep="|")
        num_tot += len(df.index)
        num_unchanged += len(df[df['status'] == 'unchanged'].index)
        num_deglycosylated += len(df[df['status'] == 'deglycosylated'].index)
        num_failed += len(df[df['status'] == 'failed'].index)
        num_error += len(df[df['status'] == 'error'].index)

    # create a dataframe with counts
    df = pd.DataFrame({"deglycosylated": [num_deglycosylated], "unchanged": [num_unchanged], "failed": [num_failed], 'error': [num_error]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_status'] = round(df['Count'] / num_tot, 3) * 100
    logger.info(f"RESULTS:\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")

    return df


def _parse_std_chunks(chunks):

    dfs = []
    for c in chunks:
        df = pd.read_csv(c, sep="|", compression="gzip").groupby("task").count()[['status']].rename({'status': 'Count'}, axis=1)
        if len(df.index) > 0:
            dfs.append(df)

    # if no case was found, return an empty dataframe
    if len(dfs) == 0:
        df = pd.DataFrame([], columns=["Count", "Category", "Perc_status"])
        return df

    # concatenate all dataframes and compute the sum of all counts
    df = pd.concat(dfs)
    df["Category"] = df.index  # I don't know how to group by index!
    df = df.groupby("Category").sum()
    df["Category"] = df.index.map(lambda x: x.replace('filter_', ''))
    num_tot = df["Count"].sum()
    df['Perc_status'] = round(df['Count'] / num_tot, 3) * 100

    return df


def plot_result_std(WD: str, output_csv: str):
    """This functions computes the CSV file for checking on the status of the standardization.
    The WD is the subfolder of the standardization (4_std) and it uses the log files for counting
    status.
    """
    logger.info("PLOT_RESULT_STD")

    # go inside log subfolder in WD
    WD_LOG = WD + "log"
    logger.info("GENERAL RESULTS")
    logger.info(f"WD={WD_LOG}")
    WD_LOG = Path(WD_LOG)

    # iterate over the log files to count status
    chunks = list(WD_LOG.glob("*[0-9][0-9][0-9]_std.log"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks)} CHUNKS")

    # initiate counts
    num_tot = 0
    num_passed = 0
    num_filtered = 0
    num_error = 0

    for c in chunks:
        df = pd.read_csv(c, sep="@", header=None)  # char not found in the log file so we can extract all lines as one column
        passed, filtered, error = [int(x) for x in df[df[0].str.contains("COUNT")].iloc[0][0].split()[-3:]]
        num_passed += passed
        num_filtered += filtered
        num_error += error
        num_tot += passed + filtered + error

    df = pd.DataFrame({"passed": [num_passed], "filtered": [num_filtered], "error": [num_error]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc_status'] = round(df['Count'] / num_tot, 3) * 100

    # display results
    logger.info(f"RESULTS:\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")

    # details

    # go inside data subfolder in WD
    WD_DATA = WD + "data"
    logger.info("DETAILED RESULTS")
    logger.info(f"WD={WD_DATA}")
    WD_DATA = Path(WD_DATA)

    # passed
    logger.info("PASSED")
    df_passed = pd.DataFrame([[num_passed, "standardized", 100.0]], columns=["Count", "Category", "Perc_status"])
    output_csv_passed = output_csv.split(".")[0] + "_passed.csv"
    if compression is not None:
        output_csv_passed += ".gz"
    df_passed.to_csv(output_csv_passed, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_passed}'")

    # filtered
    logger.info("FILTERED")
    chunks = list(WD_DATA.glob("*[0-9][0-9][0-9]_filtered.csv.gz"))
    chunks.sort()
    df_filtered = _parse_std_chunks(chunks)
    logger.info(f"RESULTS:\n{df_filtered}\n")

    output_csv_filtered = output_csv.split(".")[0] + "_filtered.csv"
    if compression is not None:
        output_csv_filtered += ".gz"
    df_filtered.to_csv(output_csv_filtered, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_filtered}'")

    # error
    logger.info("ERROR")
    chunks = list(WD_DATA.glob("*[0-9][0-9][0-9]_error.csv.gz"))
    chunks.sort()
    df_error = _parse_std_chunks(chunks)
    logger.info(f"RESULTS:\n{df_error}\n")

    output_csv_error = output_csv.split(".")[0] + "_error.csv"
    if compression is not None:
        output_csv_error += ".gz"
    df_error.to_csv(output_csv_error, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv_error}'")

    return df


def plot_result_map(WD: str, output_csv: str):
    logger.info("PLOT_RESULT_MAP")
    # go inside log subfolder in WD
    logger.info(f"WD={WD}")
    p = Path(WD)

    # iterate over the data files to count status
    chunks = list(p.glob("data/*_map/data/*[0-9][0-9][0-9]_map.csv.gz"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks)} CHUNKS_MAP")

    # concatenate all files into a single DataFrame
    df_map = pd.concat([pd.read_csv(x, sep="|", compression="gzip") for x in chunks])

    # group fragment maps into mols
    df_map.sort_values(["idm", "nfrags"], ascending=True, inplace=True)  # lowest number of frags per mol

    # count the number of fragments per molecule

    # > 1 fragments
    chunks = list(p.glob("data/*_sub/data/*[0-9][0-9][0-9]_sub.csv.gz"))
    chunks.sort()
    df_sub = pd.concat([pd.read_csv(x, sep='|', compression='gzip') for x in chunks])
    num_sub2 = len(df_sub.groupby('idm').filter(lambda x: len(x) > 1).groupby("idm").first().index)
    # num_sub2 = len(df_mols.index)
    logger.info(f"NUMBER OF MOLECULES WITH >1 NATURAL FRAGMENTS: {num_sub2}")

    # 1 fragment: filter results in sub
    chunks = list(p.glob("data/*_sub/data/*[0-9][0-9][0-9]_sub.csv.gz"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks)} CHUNKS_SUB")
    num_sub1 = len(df_sub.groupby('idm').filter(lambda x: len(x) == 1).groupby("idm").first().index)
    logger.info(f"NUMBER OF MOLECULES WITH 1 NATURAL FRAGMENT: {num_sub1}")

    # num_tot
    logger.info("COMPUTING TOTAL NUMBER OF MOLECULES")
    chunks = list(p.glob("data/*_synth/data/*[0-9][0-9][0-9]_synth.csv.gz"))
    logger.info(f"FOUND {len(chunks)} CHUNKS_SYNTH")
    # if no chunks found in synth (DNP, ect.), then use std instead
    if len(chunks) == 0:
        chunks = list(p.glob("data/*_std/data/*[0-9][0-9][0-9]_passed.csv.gz"))
        logger.info(f"FOUND {len(chunks)} CHUNKS_STD. USING THEM INSTEAD OF CHUNKS_SYNTH")
    num_tot = len(pd.concat([pd.read_csv(x, sep='|', compression='gzip') for x in chunks]).index)

    # 0 fragment: num_tot - num_sub1 - num_sub2
    num_sub0 = num_tot - num_sub1 - num_sub2
    logger.info(f"NUMBER OF MOLECULES WITH NO NATURAL FRAGMENT: {num_sub0}")

    # TODO: groupby nfrags, add the counts for 0 and 1 to these

    sys.exit(0)

    pass


def plot_result_pnp(WD: str, output_csv: str):

    logger.info("PLOT_RESULT_PNP")
    # go inside log subfolder in WD
    WD = WD + "data"
    logger.info(f"WD={WD}")
    WD = Path(WD)

    # iterate over the data files to count status
    chunks = list(WD.glob("*[0-9][0-9][0-9]_pnp.csv.gz"))
    chunks.sort()
    logger.info(f"FOUND {len(chunks)} CHUNKS")

    # initiate counts
    num_pnp_fm = 0
    num_non_pnp_fm = 0
    num_pnp_mol = 0
    num_non_pnp_mol = 0

    # begin to count
    for c in chunks:
        # fragment maps
        df = pd.read_csv(c, sep="|", compression="gzip")
        num_pnp_fm += len(df[df['pnp_fm']].index)
        num_non_pnp_fm += len(df[~df['pnp_fm']].index)
        # mols
        df.sort_values(["idm", "pnp_mol"], ascending=False, inplace=True)  # first member of a group is set to True
        df_mols = df.groupby("idm", sort=False).first()  #

        num_pnp_mol += len(df_mols[df_mols["pnp_mol"]].index)
        num_non_pnp_mol += len(df_mols[~df_mols["pnp_mol"]].index)

    # totals
    num_tot_fm = num_pnp_fm + num_non_pnp_fm
    logger.info(f"TOTAL NUMBER OF FRAGMENT MAPS: {num_tot_fm}")
    num_tot_mol = num_pnp_mol + num_non_pnp_mol
    logger.info(f"TOTAL NUMBER OF MOLECULES: {num_tot_mol}")

    # result
    df = pd.DataFrame({"PNP_fm": [num_pnp_fm], "Non-PNP_fm": [num_non_pnp_fm], "PNP_mol": [num_pnp_mol], "Non-PNP_mol": [num_non_pnp_mol]}).T.rename({0: "Count"}, axis=1)
    df['Category'] = df.index
    df['Perc'] = [round(x/num_tot_mol, 3) * 100 if "_mol" in y else round(x/num_tot_fm, 3) * 100 for x, y in zip(df["Count"], df['Category'])]
    logger.info(f"RESULTS:\n{df}\n")

    # export to csv
    format, compression = utils.get_file_format(output_csv)
    df.to_csv(output_csv, sep="|", compression=compression, index=False)
    logger.info(f"SAVED RESULTS AT '{output_csv}'")


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="Compute all required files for analyzing FCC results", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('wd', type=str, default='./', help="Working directory from the job to analyse (root folder, where the smk file is)")
    parser.add_argument('--log', type=str, default='INFO', help="Specify level of logging. Possible values are: CRITICAL, ERROR, WARNING, INFO, DEBUG.")
    args = parser.parse_args()

    # logging
    logger = utils._configure_logger(args.log)
    logger.info("RUNNING PLOT_PIPELINE_RESULTS")
    warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)  # if None is returned instead of a molecule, do not complain about mixed types
    pad = 40
    lg = RDLogger.logger()
    lg.setLevel(RDLogger.INFO)

    # define WD
    if args.wd.endswith("/"):
        wd = args.wd
    else:
        wd = args.wd + "/"
    p = Path(wd + "figures")
    if not p.is_dir():
        logger.warning(f"WARNING! OUTPUT SUBFOLDER MISSING, SO CREATING IT AT: {wd + 'figures'}")
        p.mkdir(parents=True)
    else:
        logger.info(f"OUTPUT SUBFOLDER LOCATED AT: {wd + 'figures'}")

    dataset = wd.split("/")[-2]
    logger.info(f"DATASET='{dataset}'")

    # logger.info("=" * pad)
    # logger.info("CHECKING FOR MISSING CHUNKS".center(pad))
    # logger.info("=" * pad)
    # check_chunks(wd)
    #
    # logger.info("=" * pad)
    # logger.info("CHECKING RESULTS FROM DEGLYCOSYLATION".center(pad))
    # logger.info("=" * pad)
    # subfolder_deglyco = [str(x) for x in list(Path(wd).glob("data/*_deglyco"))][0] + "/"
    # plot_result_deglyco(subfolder_deglyco, wd + f"figures/{dataset}_deglyco.csv")
    #
    # logger.info("=" * pad)
    # logger.info("CHECKING RESULTS FROM STANDARDIZATION".center(pad))
    # logger.info("=" * pad)
    # try:
    #     subfolder_std = [str(x) for x in list(Path(wd).glob("data/*_std"))][0] + "/"
    #     plot_result_std(subfolder_std, wd + f"figures/{dataset}_std.csv")
    # except IndexError:
    #     logger.info("NO STANDARDIZATION RESULTS TO CHECK")

    logger.info("=" * pad)
    logger.info("CHECKING RESULTS FROM MAPPING")
    logger.info("=" * pad)
    try:
        plot_result_map(wd, wd + f"figures/{dataset}_map.csv")
    except IndexError:
        logger.info("NO MAP RESULTS TO CHECK")

    # logger.info("=" * pad)
    # logger.info("CHECKING RESULTS FROM PNP".center(pad))
    # logger.info("=" * pad)
    # try:
    #     subfolder_pnp = [str(x) for x in list(Path(wd).glob("data/*_pnp"))][0] + "/"
    #     plot_result_pnp(subfolder_pnp, wd + f"figures/{dataset}_pnp.csv")
    # except IndexError:
    #     logger.info("NO PNP RESULTS TO CHECK")
